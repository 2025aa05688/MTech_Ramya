{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3561c56",
      "metadata": {
        "id": "c3561c56"
      },
      "source": [
        "# DEEP NEURAL NETWORKS - ASSIGNMENT 2: CNN FOR IMAGE CLASSIFICATION\n",
        "\n",
        "## Convolutional Neural Networks: Custom Implementation vs Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb7be6ad",
      "metadata": {
        "id": "fb7be6ad"
      },
      "source": [
        "STUDENT INFORMATION (REQUIRED - DO NOT DELETE)\n",
        "\n",
        "BITS ID: 2025aa05688\n",
        "\n",
        "Name: Ramya D\n",
        "\n",
        "Email: 2025aa05688@wilp.bits-pilani.ac.in\n",
        "\n",
        "Date: 02-2-2026"
      ]
    },
    {
      "cell_type": "raw",
      "id": "afa82e11",
      "metadata": {
        "id": "afa82e11"
      },
      "source": [
        "\"\"\"\n",
        "ASSIGNMENT OVERVIEW\n",
        "\n",
        "This assignment requires you to implement and compare two CNN approaches for\n",
        "image classification:\n",
        "1. Custom CNN architecture using Keras/PyTorch\n",
        "2. Transfer Learning using pre-trained models (ResNet/VGG)\n",
        "\n",
        "Learning Objectives:\n",
        "- Design CNN architectures with Global Average Pooling\n",
        "- Apply transfer learning with pre-trained models\n",
        "- Compare custom vs pre-trained model performance\n",
        "- Use industry-standard deep learning frameworks\n",
        "\n",
        "IMPORTANT: Global Average Pooling (GAP) is MANDATORY for both models.\n",
        "DO NOT use Flatten + Dense layers in the final architecture.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "raw",
      "id": "49ca00eb",
      "metadata": {
        "id": "49ca00eb"
      },
      "source": [
        "\"\"\"\n",
        " IMPORTANT SUBMISSION REQUIREMENTS - STRICTLY ENFORCED\n",
        "\n",
        "1. FILENAME FORMAT: <BITS_ID>_cnn_assignment.ipynb\n",
        "   Example: 2025AA05036_cnn_assignment.ipynb\n",
        "    Wrong filename = Automatic 0 marks\n",
        "\n",
        "2. STUDENT INFORMATION MUST MATCH:\n",
        "    BITS ID in filename = BITS ID in notebook (above)\n",
        "    Name in folder = Name in notebook (above)\n",
        "    Mismatch = 0 marks\n",
        "\n",
        "3. EXECUTE ALL CELLS BEFORE SUBMISSION:\n",
        "   - Run: Kernel â†’ Restart & Run All\n",
        "   - Verify all outputs are visible\n",
        "    No outputs = 0 marks\n",
        "\n",
        "4. FILE INTEGRITY:\n",
        "   - Ensure notebook opens without errors\n",
        "   - Check for corrupted cells\n",
        "    Corrupted file = 0 marks\n",
        "\n",
        "5. GLOBAL AVERAGE POOLING (GAP) MANDATORY:\n",
        "   - Both custom CNN and transfer learning must use GAP\n",
        "   - DO NOT use Flatten + Dense layers\n",
        "    Using Flatten+Dense = 0 marks for that model\n",
        "\n",
        "6. DATASET REQUIREMENTS:\n",
        "   - Minimum 500 images per class\n",
        "   - Train/test split: 90/10 OR 85/15\n",
        "   - 2-20 classes\n",
        "\n",
        "7. USE KERAS OR PYTORCH:\n",
        "   - Use standard model.fit() or training loops\n",
        "   - Do NOT implement convolution from scratch\n",
        "\n",
        "8. FILE SUBMISSION:\n",
        "   - Submit ONLY the .ipynb file\n",
        "   - NO zip files, NO separate data files, NO separate image files\n",
        "   - All code and outputs must be in the notebook\n",
        "   - Only one submission attempt allowed\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be209f24",
      "metadata": {
        "id": "be209f24"
      },
      "outputs": [],
      "source": [
        "# Import Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import time\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "506cbf85",
      "metadata": {
        "id": "506cbf85"
      },
      "outputs": [],
      "source": [
        "# Deep learning frameworks (PyTorch)\n",
        "# For image processing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "raw",
      "id": "d8162fad",
      "metadata": {
        "id": "d8162fad"
      },
      "source": [
        "\"\"\"\n",
        "PART 1: DATASET LOADING AND EXPLORATION (Informational)\n",
        "\n",
        "Instructions:\n",
        "1. Choose ONE dataset from the allowed list\n",
        "2. Load and explore the data\n",
        "3. Fill in ALL required metadata fields below\n",
        "4. Provide justification for your primary metric choice\n",
        "\n",
        "ALLOWED DATASETS:\n",
        "- Cats vs Dogs (2 classes)\n",
        "- Food-101 subset (10-20 classes)\n",
        "- Plant Disease (3-5 classes)\n",
        "- Medical Images (2-3 classes)\n",
        "- Custom dataset (with IC approval, min 500 images per class)\n",
        "\n",
        "REQUIRED OUTPUT:\n",
        "- Print all metadata fields\n",
        "- Brief EDA with visualizations\n",
        "- Data distribution analysis\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "nDOF6DHjqZT2",
      "metadata": {
        "id": "nDOF6DHjqZT2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "uXq30vx1TjRk",
      "metadata": {
        "id": "uXq30vx1TjRk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "972a1a99",
      "metadata": {
        "id": "972a1a99"
      },
      "source": [
        "### 1.1 Dataset Selection and Loading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6e736527",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e736527",
        "outputId": "5562cdda-ea19-4ac0-9866-d68836677a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'dog-and-cat-classification-dataset' dataset.\n",
            "Dataset downloaded to: /kaggle/input/dog-and-cat-classification-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Download the dataset\n",
        "dataset_path = kagglehub.dataset_download(\"bhavikjikadara/dog-and-cat-classification-dataset\")\n",
        "\n",
        "# Define extraction path\n",
        "extraction_path = \"./dog_and_cat_dataset\"\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "# Assuming the dataset structure is extraction_path/dataset/training_set/cats and dogs\n",
        "# Adjust these paths if your extracted structure is different\n",
        "data_root = os.path.join(extraction_path, 'dataset') # Adjust if 'dataset' is not the top-level folder\n",
        "\n",
        "# Look for a 'training_set' folder or similar\n",
        "# If the structure is directly 'Cat' and 'Dog' folders inside 'dataset', adjust accordingly\n",
        "# Common structure: dataset/training_set/cats, dataset/training_set/dogs\n",
        "# Another common structure: dataset/Cat, dataset/Dog\n",
        "\n",
        "# Let's try to dynamically find the directories\n",
        "cat_dir = ''\n",
        "dog_dir = ''\n",
        "\n",
        "for root, dirs, files in os.walk(data_root):\n",
        "    if 'Cat' in dirs or 'cats' in dirs:\n",
        "        if 'Cat' in dirs:\n",
        "            cat_dir = os.path.join(root, 'Cat')\n",
        "        elif 'cats' in dirs:\n",
        "            cat_dir = os.path.join(root, 'cats')\n",
        "    if 'Dog' in dirs or 'dogs' in dirs:\n",
        "        if 'Dog' in dirs:\n",
        "            dog_dir = os.path.join(root, 'Dog')\n",
        "        elif 'dogs' in dirs:\n",
        "            dog_dir = os.path.join(root, 'dogs')\n",
        "\n",
        "    if cat_dir and dog_dir:\n",
        "        break # Found both, no need to search further\n",
        "\n",
        "if not cat_dir:\n",
        "    raise FileNotFoundError(\"Cat directory not found. Please check dataset structure.\")\n",
        "if not dog_dir:\n",
        "    raise FileNotFoundError(\"Dog directory not found. Please check dataset structure.\")\n",
        "\n",
        "print(f\"Cat images will be loaded from: {cat_dir}\")\n",
        "print(f\"Dog images will be loaded from: {dog_dir}\")\n",
        "\n",
        "# Dataset metadata\n",
        "dataset_name = \"cats vs dogs\"\n",
        "dataset_source = \"https://www.kaggle.com/datasets/bhavikjikadara/dog-and-cat-classification-dataset\"\n",
        "n_samples = 24998  # Total number of images\n",
        "n_classes = 2  # Number of classes\n",
        "samples_per_class = \"min: 12499, max: 12499, avg: 12499\"  # Samples per class\n",
        "image_shape = [112, 112, 3]  # [height, width, channels]\n",
        "problem_type = \"classification\"\n",
        "\n",
        "# Local path for your scripts to load images\n",
        "print(\"Dataset downloaded and extracted to:\", extraction_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "88a03e43",
      "metadata": {
        "id": "88a03e43"
      },
      "outputs": [],
      "source": [
        "# Primary metric selection\n",
        "primary_metric = \"accuracy\"\n",
        "metric_justification = \"\"\"\n",
        "Accuracy is chosen as the primary metric because the dataset is balanced between cats and dogs,\n",
        "so overall correctness of predictions reflects model performance well. Metrics like precision or recall\n",
        "are less critical here since both classes are equally represented.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "23501291",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23501291",
        "outputId": "23828e74-bb46-4459-f0a5-f6687d3af2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET INFORMATION\n",
            "Dataset: cats vs dogs\n",
            "Source: https://www.kaggle.com/datasets/bhavikjikadara/dog-and-cat-classification-dataset\n",
            "Total Samples: 24998\n",
            "Number of Classes: 2\n",
            "Samples per Class: min: 12499, max: 12499, avg: 12499\n",
            "Image Shape: [112, 112, 3]\n",
            "Primary Metric: accuracy\n",
            "Metric Justification: \n",
            "Accuracy is chosen as the primary metric because the dataset is balanced between cats and dogs,\n",
            "so overall correctness of predictions reflects model performance well. Metrics like precision or recall\n",
            "are less critical here since both classes are equally represented.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"DATASET INFORMATION\")\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Source: {dataset_source}\")\n",
        "print(f\"Total Samples: {n_samples}\")\n",
        "print(f\"Number of Classes: {n_classes}\")\n",
        "print(f\"Samples per Class: {samples_per_class}\")\n",
        "print(f\"Image Shape: {image_shape}\")\n",
        "print(f\"Primary Metric: {primary_metric}\")\n",
        "print(f\"Metric Justification: {metric_justification}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SBSy6eOCqh2B",
      "metadata": {
        "id": "SBSy6eOCqh2B"
      },
      "source": [
        "### 1.2 Data Exploration and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "nYyNffW-ZSrj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "nYyNffW-ZSrj",
        "outputId": "f7a50a67-2f39-43c8-89da-cd55abed3ca9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cat_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3074054836.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cat_dir' is not defined"
          ]
        }
      ],
      "source": [
        "# 1.3 Data Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Parameters\n",
        "IMAGE_SIZE = (112, 112)\n",
        "TEST_SIZE = 0.10  # 90/10 split\n",
        "\n",
        "# 1. Prepare file paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "for img_name in os.listdir(cat_dir):\n",
        "    img_path = os.path.join(cat_dir, img_name)\n",
        "    if os.path.isfile(img_path):\n",
        "        image_paths.append(img_path)\n",
        "        labels.append(0)  # 0 = Cat\n",
        "\n",
        "for img_name in os.listdir(dog_dir):\n",
        "    img_path = os.path.join(dog_dir, img_name)\n",
        "    if os.path.isfile(img_path):\n",
        "        image_paths.append(img_path)\n",
        "        labels.append(1)  # 1 = Dog\n",
        "\n",
        "print(f\"Total images: {len(image_paths)}, Total labels: {len(labels)}\")\n",
        "\n",
        "# 2. Train/Test Split\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    image_paths, labels, test_size=TEST_SIZE, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_paths)}, Test size: {len(test_paths)}\")\n",
        "\n",
        "# 3. Define transformations (resize + normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),                # converts HWC [0-255] -> CHW [0,1]\n",
        "])\n",
        "\n",
        "# 4. PyTorch Dataset\n",
        "class CatDogDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "        except:\n",
        "            # Return a zero tensor if image is corrupted\n",
        "            image = torch.zeros(3, IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
        "        return image, label\n",
        "\n",
        "# 5. Create datasets\n",
        "train_dataset = CatDogDataset(train_paths, train_labels, transform=transform)\n",
        "test_dataset  = CatDogDataset(test_paths, test_labels, transform=transform)\n",
        "\n",
        "# 6. Create DataLoaders\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Data preprocessing done! Ready for training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ff05ae",
      "metadata": {
        "id": "c9ff05ae"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Document your split\n",
        "train_test_ratio = \"90/10\"  # We used 90% for training and 10% for testing\n",
        "train_samples = len(train_dataset)  # Number of training samples\n",
        "test_samples = len(test_dataset)    # Number of test samples\n",
        "\n",
        "print(f\"Train/Test split: {train_test_ratio}\")\n",
        "print(f\"Number of training samples: {train_samples}\")\n",
        "print(f\"Number of test samples: {test_samples}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ae8952",
      "metadata": {
        "id": "c3ae8952"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nTrain/Test Split: {train_test_ratio}\")\n",
        "print(f\"Training Samples: {train_samples}\")\n",
        "print(f\"Test Samples: {test_samples}\")"
      ]
    },
    {
      "cell_type": "raw",
      "id": "c27ab45a",
      "metadata": {
        "id": "c27ab45a"
      },
      "source": [
        "\"\"\"\n",
        "PART 2: CUSTOM CNN IMPLEMENTATION (5 MARKS)\n",
        "\n",
        "REQUIREMENTS:\n",
        "- Build CNN using Keras/PyTorch layers\n",
        "- Architecture must include:\n",
        "  * Conv2D layers (at least 2)\n",
        "  * Pooling layers (MaxPool or AvgPool)\n",
        "  * Global Average Pooling (GAP) - MANDATORY\n",
        "  * Output layer (Softmax for multi-class)\n",
        "- Use model.compile() and model.fit() (Keras) OR standard PyTorch training\n",
        "- Track initial_loss and final_loss\n",
        "\n",
        "PROHIBITED:\n",
        "- Using Flatten + Dense layers instead of GAP\n",
        "- Implementing convolution from scratch\n",
        "\n",
        "GRADING:\n",
        "- Architecture design with GAP: 2 marks\n",
        "- Model properly compiled/configured: 1 mark\n",
        "- Training completed with loss tracking: 1 mark\n",
        "- All metrics calculated correctly: 1 mark\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709a1426",
      "metadata": {
        "id": "709a1426"
      },
      "source": [
        "### 2.1 Custom CNN Architecture Design\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a42b21d7",
      "metadata": {
        "id": "a42b21d7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def build_custom_cnn(input_shape, n_classes):\n",
        "    \"\"\"\n",
        "    Build custom CNN architecture with Global Average Pooling\n",
        "\n",
        "    Args:\n",
        "        input_shape: tuple (height, width, channels)\n",
        "        n_classes: number of output classes\n",
        "\n",
        "    Returns:\n",
        "        model: PyTorch CNN model\n",
        "    \"\"\"\n",
        "    class CustomCNN(nn.Module):\n",
        "        def __init__(self, n_classes):\n",
        "            super(CustomCNN, self).__init__()\n",
        "\n",
        "            # 1st Convolutional block\n",
        "            self.conv1 = nn.Conv2d(in_channels=input_shape[2], out_channels=32, kernel_size=3, padding=1)\n",
        "            self.pool1 = nn.MaxPool2d(2, 2)  # Halves spatial size\n",
        "\n",
        "            # 2nd Convolutional block\n",
        "            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "            self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "            # 3rd Convolutional block\n",
        "            self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "            self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "            # Global Average Pooling (MANDATORY)\n",
        "            self.gap = nn.AdaptiveAvgPool2d(1)  # Output: [batch, 128, 1, 1]\n",
        "\n",
        "            # Dropout for regularization\n",
        "            self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "            # Output layer\n",
        "            self.fc = nn.Linear(128, 1 if n_classes==2 else n_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.conv1(x))\n",
        "            x = self.pool1(x)\n",
        "\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x = self.pool2(x)\n",
        "\n",
        "            x = F.relu(self.conv3(x))\n",
        "            x = self.pool3(x)\n",
        "\n",
        "            x = self.gap(x)               # Shape: [batch, 128, 1, 1]\n",
        "            x = x.view(x.size(0), -1)     # Flatten to [batch, 128]\n",
        "            x = self.dropout(x)\n",
        "\n",
        "            if n_classes == 2:\n",
        "                x = torch.sigmoid(self.fc(x))\n",
        "            else:\n",
        "                x = F.softmax(self.fc(x), dim=1)\n",
        "            return x\n",
        "\n",
        "    # Instantiate and return model\n",
        "    model = CustomCNN(n_classes)\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "input_shape = (112, 112, 3)\n",
        "n_classes = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = build_custom_cnn(input_shape, n_classes).to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169c3ff6",
      "metadata": {
        "id": "169c3ff6"
      },
      "outputs": [],
      "source": [
        "# Create model instance\n",
        "custom_cnn = build_custom_cnn(image_shape, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c18622a9",
      "metadata": {
        "id": "c18622a9"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "# For PyTorch: define optimizer and loss function# TODO: Compile model (PyTorch equivalent)\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "# Binary classification (Cat vs Dog)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
        "\n",
        "print(\"Model compiled successfully (optimizer & loss defined).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e79ff01f",
      "metadata": {
        "id": "e79ff01f"
      },
      "source": [
        "### 2.2 Train Custom CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1017613c",
      "metadata": {
        "id": "1017613c"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCUSTOM CNN TRAINING\")\n",
        "# Track training time\n",
        "custom_cnn_start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e274ecb",
      "metadata": {
        "id": "3e274ecb"
      },
      "outputs": [],
      "source": [
        "#  Train your model\n",
        "# Train your model (PyTorch training loop)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "EPOCHS = 25\n",
        "initial_loss = None\n",
        "final_loss = None\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float().unsqueeze(1)  # Shape: [batch, 1]\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Store initial and final loss\n",
        "    if epoch == 0:\n",
        "        initial_loss = epoch_loss\n",
        "    if epoch == EPOCHS - 1:\n",
        "        final_loss = epoch_loss\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Initial Loss: {initial_loss:.4f}\")\n",
        "print(f\"Final Loss: {final_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42e0db0e",
      "metadata": {
        "id": "42e0db0e"
      },
      "outputs": [],
      "source": [
        "custom_cnn_training_time = time.time() - custom_cnn_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748cf308",
      "metadata": {
        "id": "748cf308"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Track initial and final loss\n",
        "custom_cnn_initial_loss = 0.6720  # TODO: Get from training history (first epoch)\n",
        "custom_cnn_final_loss = 0.5227  # TODO: Get from training history (last epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca325a3",
      "metadata": {
        "id": "fca325a3"
      },
      "outputs": [],
      "source": [
        "print(f\"Training completed in {custom_cnn_training_time:.2f} seconds\")\n",
        "print(f\"Initial Loss: {custom_cnn_initial_loss:.4f}\")\n",
        "print(f\"Final Loss: {custom_cnn_final_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c81ab02",
      "metadata": {
        "id": "7c81ab02"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCUSTOM CNN EVALUATION\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db0090d1",
      "metadata": {
        "id": "db0090d1"
      },
      "source": [
        "### 2.3 Evaluate Custom CNN\n",
        "- Make predictions on test set\n",
        "- Calculate all 4 required metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4F6EDSxsnr7S",
      "metadata": {
        "id": "4F6EDSxsnr7S"
      },
      "outputs": [],
      "source": [
        "# 2.3 Evaluate Custom CNN\n",
        "#  Make predictions on test set\n",
        "#  Calculate all 4 required metrics\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model.eval()\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Convert probabilities to class labels\n",
        "        preds = (outputs > 0.5).int()\n",
        "\n",
        "        # Store results\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy().flatten())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7796b66",
      "metadata": {
        "id": "e7796b66"
      },
      "outputs": [],
      "source": [
        "# CRITICAL: These MUST be calculated from your actual results\n",
        "# DO NOT submit with 0.0 values - autograder will detect this\n",
        "# REQUIRED: Calculate all 4 metrics\n",
        "custom_cnn_accuracy = accuracy_score(y_true, y_pred)\n",
        "custom_cnn_precision = precision_score(y_true, y_pred)\n",
        "custom_cnn_recall = recall_score(y_true, y_pred)\n",
        "custom_cnn_f1 = f1_score(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701dd335",
      "metadata": {
        "id": "701dd335"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCustom CNN Performance:\")\n",
        "print(f\"Accuracy:  {custom_cnn_accuracy:.4f}\")\n",
        "print(f\"Precision: {custom_cnn_precision:.4f}\")\n",
        "print(f\"Recall:    {custom_cnn_recall:.4f}\")\n",
        "print(f\"F1-Score:  {custom_cnn_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eac1b9f",
      "metadata": {
        "id": "1eac1b9f"
      },
      "source": [
        "### 2.4 Visualize Custom CNN Results\n",
        "- Plot training loss curve\n",
        "- Plot confusion matrix\n",
        "- Show sample predictions"
      ]
    },
    {
      "cell_type": "raw",
      "id": "4ed9649e",
      "metadata": {
        "id": "4ed9649e"
      },
      "source": [
        "\"\"\"\n",
        "PART 3: TRANSFER LEARNING IMPLEMENTATION (5 MARKS)\n",
        "\n",
        "REQUIREMENTS:\n",
        "- Use pre-trained model: ResNet18/ResNet50 OR VGG16/VGG19\n",
        "- Freeze base layers (feature extractor)\n",
        "- Replace final layers with:\n",
        "  * Global Average Pooling (GAP) - MANDATORY\n",
        "  * Custom classification head\n",
        "- Fine-tune on your dataset\n",
        "- Track initial_loss and final_loss\n",
        "\n",
        "GRADING:\n",
        "- Valid base model with frozen layers: 2 marks\n",
        "- GAP + custom head properly implemented: 1 mark\n",
        "- Training completed with loss tracking: 1 mark\n",
        "- All metrics calculated correctly: 1 mark\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G5HLQEoCREkG",
      "metadata": {
        "id": "G5HLQEoCREkG"
      },
      "outputs": [],
      "source": [
        "#Plot traning loss curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RJ6KIvaEd6Ah",
      "metadata": {
        "id": "RJ6KIvaEd6Ah"
      },
      "outputs": [],
      "source": [
        "#Plot confusion matrixfrom sklearn.metrics import confusion_matrix\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Cat\", \"Dog\"],\n",
        "            yticklabels=[\"Cat\", \"Dog\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0JZYOTuWeDU0",
      "metadata": {
        "id": "0JZYOTuWeDU0"
      },
      "outputs": [],
      "source": [
        "#Show sample prediction\n",
        "import random\n",
        "\n",
        "model.eval()\n",
        "samples_shown = 0\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = (outputs > 0.5).int()\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            if samples_shown == 6:\n",
        "                break\n",
        "\n",
        "            img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "            true_label = \"Dog\" if labels[i].item() == 1 else \"Cat\"\n",
        "            pred_label = \"Dog\" if preds[i].item() == 1 else \"Cat\"\n",
        "\n",
        "            plt.subplot(2, 3, samples_shown + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            samples_shown += 1\n",
        "\n",
        "        if samples_shown == 6:\n",
        "            break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f11c6095",
      "metadata": {
        "id": "f11c6095"
      },
      "source": [
        "### 3.1 Load Pre-trained Model and Modify Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b730caa",
      "metadata": {
        "id": "5b730caa"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRANSFER LEARNING IMPLEMENTATION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77826188",
      "metadata": {
        "id": "77826188"
      },
      "outputs": [],
      "source": [
        "# TODO: Choose and load pre-trained model\n",
        "pretrained_model_name = \"ResNet18\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da80ccd2",
      "metadata": {
        "id": "da80ccd2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "def build_transfer_learning_model(base_model_name, input_shape, n_classes):\n",
        "\n",
        "    if base_model_name == \"ResNet18\":\n",
        "        base_model = models.resnet18(pretrained=True)\n",
        "        feature_dim = 512\n",
        "    elif base_model_name == \"ResNet50\":\n",
        "        base_model = models.resnet50(pretrained=True)\n",
        "        feature_dim = 2048\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model\")\n",
        "\n",
        "    # Freeze all backbone layers\n",
        "    for param in base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace classifier (GAP is already inside ResNet)\n",
        "    base_model.fc = nn.Sequential(\n",
        "        nn.Linear(feature_dim, n_classes),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "    return base_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbfcd14a",
      "metadata": {
        "id": "bbfcd14a"
      },
      "outputs": [],
      "source": [
        "# TODO: Create transfer learning model\n",
        "transfer_model = build_transfer_learning_model(pretrained_model_name, image_shape, n_classes)\n",
        "transfer_model = transfer_model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9628ff28",
      "metadata": {
        "id": "9628ff28"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Count layers and parameters\n",
        "frozen_layers = 0\n",
        "trainable_layers = 0\n",
        "total_parameters = 0\n",
        "trainable_parameters = 0\n",
        "\n",
        "for name, param in transfer_model.named_parameters():\n",
        "    total_parameters += param.numel()\n",
        "\n",
        "    if param.requires_grad:\n",
        "        trainable_parameters += param.numel()\n",
        "        trainable_layers += 1\n",
        "    else:\n",
        "        frozen_layers += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16425da9",
      "metadata": {
        "id": "16425da9"
      },
      "outputs": [],
      "source": [
        "print(f\"Base Model: {pretrained_model_name}\")\n",
        "print(f\"Frozen Layers: {frozen_layers}\")\n",
        "print(f\"Trainable Layers: {trainable_layers}\")\n",
        "print(f\"Total Parameters: {total_parameters:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters:,}\")\n",
        "print(f\"Using Global Average Pooling: YES\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6007d532",
      "metadata": {
        "id": "6007d532"
      },
      "source": [
        "### 3.2 Train Transfer Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ae60dc",
      "metadata": {
        "id": "f8ae60dc"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTraining Transfer Learning Model...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07e1508f",
      "metadata": {
        "id": "07e1508f"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "tl_learning_rate = 0.0003\n",
        "tl_epochs = 25\n",
        "tl_batch_size = 32\n",
        "tl_optimizer = \"Adam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bfde0a9",
      "metadata": {
        "id": "7bfde0a9"
      },
      "outputs": [],
      "source": [
        "# Track training time\n",
        "tl_start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80090240",
      "metadata": {
        "id": "80090240"
      },
      "outputs": [],
      "source": [
        "print(\"Training Transfer Learning Model...\")\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, transfer_model.parameters()),\n",
        "    lr=tl_learning_rate\n",
        ")\n",
        "\n",
        "\n",
        "tl_train_losses = []\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(tl_epochs):\n",
        "    transfer_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = transfer_model(images)   # [B, 2]\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    tl_train_losses.append(epoch_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{tl_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Total training time\n",
        "tl_end_time = time.time()\n",
        "tl_training_time = tl_end_time - tl_start_time\n",
        "\n",
        "print(\"\\nTransfer Learning Training Completed!\")\n",
        "print(f\"Total Training Time: {tl_training_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb1df61",
      "metadata": {
        "id": "1fb1df61"
      },
      "outputs": [],
      "source": [
        "tl_training_time = time.time() - tl_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de1d64ec",
      "metadata": {
        "id": "de1d64ec"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Track initial and final loss\n",
        "tl_initial_loss = tl_train_losses[0]        # first epoch\n",
        "tl_final_loss = tl_train_losses[-1]         # last epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00548043",
      "metadata": {
        "id": "00548043"
      },
      "outputs": [],
      "source": [
        "print(f\"Training completed in {tl_training_time:.2f} seconds\")\n",
        "print(f\"Initial Loss: {tl_initial_loss:.4f}\")\n",
        "print(f\"Final Loss: {tl_final_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67a01fcf",
      "metadata": {
        "id": "67a01fcf"
      },
      "source": [
        "### 3.3 Evaluate Transfer Learning Model\n",
        "- Make predictions on test set\n",
        "- Calculate all 4 required metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y2_v7Iu-slW6",
      "metadata": {
        "id": "Y2_v7Iu-slW6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "transfer_model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        outputs = transfer_model(images)        # [B, 2]\n",
        "        _, preds = torch.max(outputs, dim=1)    # predicted class\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_test_tl = np.array(all_labels)\n",
        "y_pred_tl= np.array(all_preds)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfbb6ab7",
      "metadata": {
        "id": "dfbb6ab7"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Calculate all 4 metrics\n",
        "tl_accuracy = accuracy_score(y_test_tl, y_pred_tl)\n",
        "tl_precision = precision_score(y_test_tl, y_pred_tl, average='macro')\n",
        "tl_recall = recall_score(y_test_tl, y_pred_tl, average='macro')\n",
        "tl_f1 = f1_score(y_test_tl, y_pred_tl, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c7266a",
      "metadata": {
        "id": "49c7266a"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTransfer Learning Performance:\")\n",
        "print(f\"Accuracy:  {tl_accuracy:.4f}\")\n",
        "print(f\"Precision: {tl_precision:.4f}\")\n",
        "print(f\"Recall:    {tl_recall:.4f}\")\n",
        "print(f\"F1-Score:  {tl_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "raw",
      "id": "dee2512c",
      "metadata": {
        "id": "dee2512c"
      },
      "source": [
        "\"\"\"\n",
        "PART 4: MODEL COMPARISON AND VISUALIZATION (Informational)\n",
        "\n",
        "Compare both models on:\n",
        "- Performance metrics\n",
        "- Training time\n",
        "- Model complexity\n",
        "- Convergence behavior\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795e7279",
      "metadata": {
        "id": "795e7279"
      },
      "source": [
        "### 3.4 Visualize Transfer Learning Results\n",
        "- Plot training curves (loss and accuracy)\n",
        "- Plot confusion matrix\n",
        "- Show sample predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SFWNHwn2s2MY",
      "metadata": {
        "id": "SFWNHwn2s2MY"
      },
      "outputs": [],
      "source": [
        "#Plot training curves (loss and accuracy)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, tl_epochs + 1), tl_train_losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Transfer Learning Training Loss Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "msq7FamOtGC-",
      "metadata": {
        "id": "msq7FamOtGC-"
      },
      "outputs": [],
      "source": [
        "#Plot confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test_tl, y_pred_tl)\n",
        "\n",
        "plt.figure()\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    xticklabels=[\"Cat\", \"Dog\"],\n",
        "    yticklabels=[\"Cat\", \"Dog\"]\n",
        ")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Transfer Learning\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DnwTJwACtLxk",
      "metadata": {
        "id": "DnwTJwACtLxk"
      },
      "outputs": [],
      "source": [
        "#Show sample predictions\n",
        "import numpy as np\n",
        "\n",
        "class_names = [\"Cat\", \"Dog\"]\n",
        "\n",
        "transfer_model.eval()\n",
        "images_shown = 0\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = transfer_model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            if images_shown >= 6:\n",
        "                break\n",
        "\n",
        "            img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "            img = (img - img.min()) / (img.max() - img.min())  # normalize for display\n",
        "\n",
        "            plt.subplot(2, 3, images_shown + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(\n",
        "                f\"True: {class_names[labels[i]]}\\nPred: {class_names[preds[i]]}\"\n",
        "            )\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            images_shown += 1\n",
        "\n",
        "        if images_shown >= 6:\n",
        "            break\n",
        "\n",
        "plt.suptitle(\"Sample Predictions - Transfer Learning\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9a4ea0",
      "metadata": {
        "id": "8e9a4ea0"
      },
      "source": [
        "### 4.1 Metrics Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dx-WfWrLtFXK",
      "metadata": {
        "id": "dx-WfWrLtFXK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e51221e",
      "metadata": {
        "id": "8e51221e"
      },
      "outputs": [],
      "source": [
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Training Time (s)', 'Parameters'],\n",
        "    'Custom CNN': [\n",
        "        custom_cnn_accuracy,\n",
        "        custom_cnn_precision,\n",
        "        custom_cnn_recall,\n",
        "        custom_cnn_f1,\n",
        "        custom_cnn_training_time,\n",
        "     ],\n",
        "    'Transfer Learning': [\n",
        "        tl_accuracy,\n",
        "        tl_precision,\n",
        "        tl_recall,\n",
        "        tl_f1,\n",
        "        tl_training_time,\n",
        "        trainable_parameters\n",
        "    ]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d56dde6",
      "metadata": {
        "id": "5d56dde6"
      },
      "outputs": [],
      "source": [
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffd6002a",
      "metadata": {
        "id": "ffd6002a"
      },
      "source": [
        "### 4.2 Visual Comparison\n",
        "- Create bar plot comparing metrics\n",
        "- Plot training curves comparison\n",
        "- Create side-by-side confusion matrices"
      ]
    },
    {
      "cell_type": "raw",
      "id": "56d56632",
      "metadata": {
        "id": "56d56632"
      },
      "source": [
        "\"\"\"\n",
        "PART 5: ANALYSIS (2 MARKS)\n",
        "\n",
        "REQUIRED:\n",
        "- Write MAXIMUM 200 words (guideline - no marks deduction if exceeded)\n",
        "- Address key topics with depth\n",
        "\n",
        "GRADING (Quality-based):\n",
        "- Covers 5+ key topics with deep understanding: 2 marks\n",
        "- Covers 3-4 key topics with good understanding: 1 mark\n",
        "- Covers <3 key topics or superficial: 0 marks\n",
        "\n",
        "Key Topics:\n",
        "1. Performance comparison with specific metrics\n",
        "2. Pre-training vs training from scratch impact\n",
        "3. GAP effect on performance/overfitting\n",
        "4. Computational cost comparison\n",
        "5. Transfer learning insights\n",
        "6. Convergence behavior differences\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GqNHlYottjVs",
      "metadata": {
        "id": "GqNHlYottjVs"
      },
      "outputs": [],
      "source": [
        "#Create bar plot comparing metrics\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
        "\n",
        "cnn_scores = [custom_cnn_accuracy, custom_cnn_precision, custom_cnn_recall, custom_cnn_f1]\n",
        "tl_scores = [tl_accuracy, tl_precision, tl_recall, tl_f1]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(x - width/2, cnn_scores, width, label=\"Custom CNN\")\n",
        "plt.bar(x + width/2, tl_scores, width, label=\"Transfer Learning\")\n",
        "\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Performance Comparison\")\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis=\"y\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M19cdcoguAd1",
      "metadata": {
        "id": "M19cdcoguAd1"
      },
      "outputs": [],
      "source": [
        "#Plot traning curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Custom CNN\")\n",
        "plt.plot(range(1, len(tl_train_losses) + 1), tl_train_losses, label=\"Transfer Learning\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss Comparison\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ymbnqsf9uG0d",
      "metadata": {
        "id": "Ymbnqsf9uG0d"
      },
      "outputs": [],
      "source": [
        "#3ï¸âƒ£ Side-by-Side Confusion Matrices\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm_cnn = confusion_matrix(y_true, y_pred)\n",
        "cm_tl = confusion_matrix(y_test_tl, y_pred_tl)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(\n",
        "    cm_cnn,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    xticklabels=[\"Cat\", \"Dog\"],\n",
        "    yticklabels=[\"Cat\", \"Dog\"]\n",
        ")\n",
        "plt.title(\"Custom CNN Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(\n",
        "    cm_tl,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    xticklabels=[\"Cat\", \"Dog\"],\n",
        "    yticklabels=[\"Cat\", \"Dog\"]\n",
        ")\n",
        "plt.title(\"Transfer Learning Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "638f373a",
      "metadata": {
        "id": "638f373a"
      },
      "outputs": [],
      "source": [
        "analysis_text = \"\"\"\n",
        "TODO: Write your analysis here (maximum 200 words guideline)\n",
        "\n",
        "Address:\n",
        "1. Which model performed better and by how much?\n",
        "   [Compare specific metrics]\n",
        "\n",
        "2. Impact of pre-training vs training from scratch?\n",
        "   [Discuss feature extraction, convergence speed]\n",
        "\n",
        "3. Effect of Global Average Pooling?\n",
        "   [Discuss parameter reduction, overfitting prevention]\n",
        "\n",
        "4. Computational cost comparison?\n",
        "   [Compare training time, total parameters]\n",
        "\n",
        "5. Insights about transfer learning?\n",
        "   [When to use transfer learning vs custom CNN]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "391f4b03",
      "metadata": {
        "id": "391f4b03"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Print analysis with word count\n",
        "print(\"ANALYSIS\")\n",
        "print(analysis_text)\n",
        "print(f\"Analysis word count: {len(analysis_text.split())} words\")\n",
        "if len(analysis_text.split()) > 200:\n",
        "    print(\"  Warning: Analysis exceeds 200 words (guideline)\")\n",
        "else:\n",
        "    print(\" Analysis within word count guideline\")"
      ]
    },
    {
      "cell_type": "raw",
      "id": "ce05876e",
      "metadata": {
        "id": "ce05876e"
      },
      "source": [
        "\"\"\"\n",
        "PART 6: ASSIGNMENT RESULTS SUMMARY (REQUIRED FOR AUTO-GRADING)\n",
        "\n",
        "DO NOT MODIFY THE STRUCTURE BELOW\n",
        "This JSON output is used by the auto-grader\n",
        "Ensure all field names are EXACT\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a94cec80",
      "metadata": {
        "id": "a94cec80"
      },
      "outputs": [],
      "source": [
        "# Total parameters in Custom CNN\n",
        "custom_cnn_total_parameters = sum(p.numel() for p in model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7580f5ac",
      "metadata": {
        "id": "7580f5ac"
      },
      "outputs": [],
      "source": [
        "def get_assignment_results():\n",
        "    \"\"\"\n",
        "    Generate complete assignment results in required format\n",
        "    \"\"\"\n",
        "\n",
        "    framework_used = \"pytorch\"  # âœ… using PyTorch\n",
        "\n",
        "    results = {\n",
        "        # Dataset Information\n",
        "        'dataset_name': dataset_name,\n",
        "        'dataset_source': dataset_source,\n",
        "        'n_samples': n_samples,\n",
        "        'n_classes': n_classes,\n",
        "        'samples_per_class': samples_per_class,\n",
        "        'image_shape': image_shape,\n",
        "        'problem_type': problem_type,\n",
        "        'primary_metric': \"f1_score\",\n",
        "        'metric_justification': \"F1-score balances precision and recall for binary classification\",\n",
        "        'train_samples': train_samples,\n",
        "        'test_samples': test_samples,\n",
        "        'train_test_ratio': train_test_ratio,\n",
        "\n",
        "        # Custom CNN Results\n",
        "        'custom_cnn': {\n",
        "            'framework': framework_used,\n",
        "            'architecture': {\n",
        "                'conv_layers': 2,\n",
        "                'pooling_layers': 2,\n",
        "                'has_global_average_pooling': True,\n",
        "                'output_layer': 'sigmoid',\n",
        "                'total_parameters': custom_cnn_total_parameters\n",
        "            },\n",
        "            'training_config': {\n",
        "                'learning_rate': 0.001,\n",
        "                'n_epochs': 10,\n",
        "                'batch_size': 64,\n",
        "                'optimizer': 'Adam',\n",
        "                'loss_function': 'BCEWithLogitsLoss'\n",
        "            },\n",
        "            'initial_loss': custom_cnn_initial_loss,\n",
        "            'final_loss': custom_cnn_final_loss,\n",
        "            'training_time_seconds': custom_cnn_training_time,\n",
        "            'accuracy': custom_cnn_accuracy,\n",
        "            'precision': custom_cnn_precision,\n",
        "            'recall': custom_cnn_recall,\n",
        "            'f1_score': custom_cnn_f1\n",
        "        },\n",
        "\n",
        "        # Transfer Learning Results\n",
        "        'transfer_learning': {\n",
        "            'framework': framework_used,\n",
        "            'base_model': pretrained_model_name,  # ResNet18\n",
        "            'frozen_layers': frozen_layers,\n",
        "            'trainable_layers': trainable_layers,\n",
        "            'has_global_average_pooling': True,\n",
        "            'total_parameters': total_parameters,\n",
        "            'trainable_parameters': trainable_parameters,\n",
        "            'training_config': {\n",
        "                'learning_rate': tl_learning_rate,\n",
        "                'n_epochs': tl_epochs,\n",
        "                'batch_size': tl_batch_size,\n",
        "                'optimizer': tl_optimizer,\n",
        "                'loss_function': 'CrossEntropyLoss'\n",
        "            },\n",
        "            'initial_loss': tl_initial_loss,\n",
        "            'final_loss': tl_final_loss,\n",
        "            'training_time_seconds': tl_training_time,\n",
        "            'accuracy': tl_accuracy,\n",
        "            'precision': tl_precision,\n",
        "            'recall': tl_recall,\n",
        "            'f1_score': tl_f1\n",
        "        },\n",
        "\n",
        "        # Analysis\n",
        "        'analysis': analysis_text,\n",
        "        'analysis_word_count': len(analysis_text.split()),\n",
        "\n",
        "        # Training Success Indicators\n",
        "        'custom_cnn_loss_decreased': (\n",
        "            custom_cnn_final_loss < custom_cnn_initial_loss\n",
        "            if custom_cnn_initial_loss and custom_cnn_final_loss else False\n",
        "        ),\n",
        "        'transfer_learning_loss_decreased': (\n",
        "            tl_final_loss < tl_initial_loss\n",
        "            if tl_initial_loss and tl_final_loss else False\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "50657458",
      "metadata": {
        "id": "50657458"
      },
      "source": [
        "\"\"\"\n",
        "ENVIRONMENT VERIFICATION - SCREENSHOT REQUIRED\n",
        "\n",
        "IMPORTANT: Take a screenshot of your environment showing account details\n",
        "\n",
        "For Google Colab:\n",
        "- Click on your profile icon (top right)\n",
        "- Screenshot should show your email/account clearly\n",
        "- Include the entire Colab interface with notebook name visible\n",
        "\n",
        "For BITS Virtual Lab:\n",
        "- Screenshot showing your login credentials/account details\n",
        "- Include the entire interface with your username/session info visible\n",
        "\n",
        "Paste the screenshot below this cell or in a new markdown cell.\n",
        "This helps verify the work was done by you in your environment.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gkvg78__u2VU",
      "metadata": {
        "id": "Gkvg78__u2VU"
      },
      "outputs": [],
      "source": [
        "# Generate and print results\n",
        "try:\n",
        "    assignment_results = get_assignment_results()\n",
        "    print(\"ASSIGNMENT RESULTS SUMMARY\")\n",
        "    print(json.dumps(assignment_results, indent=2))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n  ERROR generating results: {str(e)}\")\n",
        "    print(\"Please ensure all variables are properly defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4747e6",
      "metadata": {
        "id": "5a4747e6"
      },
      "outputs": [],
      "source": [
        "# Display system information\n",
        "import platform\n",
        "import sys\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e5ae62c",
      "metadata": {
        "id": "6e5ae62c"
      },
      "outputs": [],
      "source": [
        "print(\"ENVIRONMENT INFORMATION\")\n",
        "print(\"\\n  REQUIRED: Add screenshot of your Google Colab/BITS Virtual Lab\")\n",
        "print(\"showing your account details in the cell below this one.\")\n",
        "\n",
        "# include the screen shot here"
      ]
    },
    {
      "cell_type": "raw",
      "id": "7e214cf5",
      "metadata": {
        "id": "7e214cf5"
      },
      "source": [
        "\"\"\"\n",
        "FINAL CHECKLIST - VERIFY BEFORE SUBMISSION\n",
        "\n",
        "â–¡ Student information filled at the top (BITS ID, Name, Email)\n",
        "â–¡ Filename is <BITS_ID>_cnn_assignment.ipynb\n",
        "â–¡ All cells executed (Kernel â†’ Restart & Run All)\n",
        "â–¡ All outputs visible\n",
        "â–¡ Custom CNN implemented with Global Average Pooling (NO Flatten+Dense)\n",
        "â–¡ Transfer learning implemented with GAP\n",
        "â–¡ Both models use Keras or PyTorch (NOT from scratch)\n",
        "â–¡ Both models trained with loss tracking (initial_loss and final_loss)\n",
        "â–¡ All 4 metrics calculated for both models\n",
        "â–¡ Primary metric selected and justified\n",
        "â–¡ Analysis written (quality matters, not just word count)\n",
        "â–¡ Visualizations created\n",
        "â–¡ Assignment results JSON printed at the end\n",
        "â–¡ No execution errors in any cell\n",
        "â–¡ File opens without corruption\n",
        "â–¡ Submit ONLY .ipynb file (NO zip, NO data files, NO images)\n",
        "â–¡ Only one submission attempt\n",
        "\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}