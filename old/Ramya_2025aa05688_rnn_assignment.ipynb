{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "7dc017f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "7dc017f1",
        "outputId": "85ccf228-593f-41e9-e53b-4391fde58561"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nDEEP NEURAL NETWORKS - ASSIGNMENT 3: RNN vs TRANSFORMER FOR TIME SERIES\\nRecurrent Neural Networks vs Transformers for Time Series Prediction\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "DEEP NEURAL NETWORKS - ASSIGNMENT 3: RNN vs TRANSFORMER FOR TIME SERIES\n",
        "Recurrent Neural Networks vs Transformers for Time Series Prediction\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "23b67847",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "23b67847",
        "outputId": "b023a42e-09d9-48e6-f62c-ca8391026a67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nSTUDENT INFORMATION (REQUIRED - DO NOT DELETE)\\n================================================================================\\n\\nBITS ID: 2025AA05688\\nName: Ramya D\\nEmail: 2025aa05688@wilp.bits-pilani.ac.in\\nDate: 06-02-2026\\n\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "STUDENT INFORMATION (REQUIRED - DO NOT DELETE)\n",
        "================================================================================\n",
        "\n",
        "BITS ID: 2025AA05688\n",
        "Name: Ramya D\n",
        "Email: 2025aa05688@wilp.bits-pilani.ac.in\n",
        "Date: 06-02-2026\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "b5a0dc29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "b5a0dc29",
        "outputId": "d0f277e2-e371-4d49-d4da-b5c7cfd08dd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nASSIGNMENT OVERVIEW\\n================================================================================\\n\\nThis assignment requires you to implement and compare two approaches for \\ntime series forecasting:\\n1. LSTM or GRU using Keras/PyTorch\\n2. Transformer encoder using Keras/PyTorch layers\\n\\nLearning Objectives:\\n- Build recurrent neural networks for sequential data\\n- Use transformer architecture for time series\\n- Implement or integrate positional encoding\\n- Compare RNN vs Transformer architectures\\n- Understand time series preprocessing and evaluation\\n\\nIMPORTANT: \\n- Positional encoding MUST be added to transformer\\n- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\\n- DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\\n- Use temporal train/test split (NO shuffling)\\n\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "ASSIGNMENT OVERVIEW\n",
        "================================================================================\n",
        "\n",
        "This assignment requires you to implement and compare two approaches for\n",
        "time series forecasting:\n",
        "1. LSTM or GRU using Keras/PyTorch\n",
        "2. Transformer encoder using Keras/PyTorch layers\n",
        "\n",
        "Learning Objectives:\n",
        "- Build recurrent neural networks for sequential data\n",
        "- Use transformer architecture for time series\n",
        "- Implement or integrate positional encoding\n",
        "- Compare RNN vs Transformer architectures\n",
        "- Understand time series preprocessing and evaluation\n",
        "\n",
        "IMPORTANT:\n",
        "- Positional encoding MUST be added to transformer\n",
        "- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "- DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
        "- Use temporal train/test split (NO shuffling)\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "c4b05f5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "c4b05f5b",
        "outputId": "9953f091-498e-4ea5-dae4-c3a7e1dcd4d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n================================================================================\\n⚠️ IMPORTANT SUBMISSION REQUIREMENTS - STRICTLY ENFORCED ⚠️\\n================================================================================\\n\\n1. FILENAME FORMAT: <BITS_ID>_rnn_assignment.ipynb\\n   Example: 2025AA05036_rnn_assignment.ipynb\\n   ❌ Wrong filename = Automatic 0 marks\\n\\n2. STUDENT INFORMATION MUST MATCH:\\n   ✓ BITS ID in filename = BITS ID in notebook (above)\\n   ✓ Name in folder = Name in notebook (above)\\n   ❌ Mismatch = 0 marks\\n\\n3. EXECUTE ALL CELLS BEFORE SUBMISSION:\\n   - Run: Kernel → Restart & Run All\\n   - Verify all outputs are visible\\n   ❌ No outputs = 0 marks\\n\\n4. FILE INTEGRITY:\\n   - Ensure notebook opens without errors\\n   - Check for corrupted cells\\n   ❌ Corrupted file = 0 marks\\n\\n5. IMPLEMENTATION REQUIREMENTS:\\n   - MUST add positional encoding to transformer (custom or built-in)\\n   - CAN use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\\n   - DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\\n   - DO NOT shuffle time series data (temporal order required)\\n   ❌ Missing positional encoding = 0 marks for transformer section\\n\\n6. DATASET REQUIREMENTS:\\n   - Minimum 1000 time steps\\n   - Train/test split: 90/10 OR 85/15 (temporal split only)\\n   - Sequence length: 10-50 time steps\\n   - Prediction horizon: 1-10 time steps\\n\\n7. USE KERAS OR PYTORCH:\\n   - Use framework's LSTM/GRU layers\\n   - Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\\n   - Add positional encoding (custom implementation or built-in)\\n   - Use standard training methods\\n\\n8. FILE SUBMISSION:\\n   - Submit ONLY the .ipynb file\\n   - NO zip files, NO separate data files, NO separate image files\\n   - All code and outputs must be in the notebook\\n   - Only one submission attempt allowed\\n\\n================================================================================\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "⚠️ IMPORTANT SUBMISSION REQUIREMENTS - STRICTLY ENFORCED ⚠️\n",
        "================================================================================\n",
        "\n",
        "1. FILENAME FORMAT: <BITS_ID>_rnn_assignment.ipynb\n",
        "   Example: 2025AA05036_rnn_assignment.ipynb\n",
        "   ❌ Wrong filename = Automatic 0 marks\n",
        "\n",
        "2. STUDENT INFORMATION MUST MATCH:\n",
        "   ✓ BITS ID in filename = BITS ID in notebook (above)\n",
        "   ✓ Name in folder = Name in notebook (above)\n",
        "   ❌ Mismatch = 0 marks\n",
        "\n",
        "3. EXECUTE ALL CELLS BEFORE SUBMISSION:\n",
        "   - Run: Kernel → Restart & Run All\n",
        "   - Verify all outputs are visible\n",
        "   ❌ No outputs = 0 marks\n",
        "\n",
        "4. FILE INTEGRITY:\n",
        "   - Ensure notebook opens without errors\n",
        "   - Check for corrupted cells\n",
        "   ❌ Corrupted file = 0 marks\n",
        "\n",
        "5. IMPLEMENTATION REQUIREMENTS:\n",
        "   - MUST add positional encoding to transformer (custom or built-in)\n",
        "   - CAN use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "   - DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
        "   - DO NOT shuffle time series data (temporal order required)\n",
        "   ❌ Missing positional encoding = 0 marks for transformer section\n",
        "\n",
        "6. DATASET REQUIREMENTS:\n",
        "   - Minimum 1000 time steps\n",
        "   - Train/test split: 90/10 OR 85/15 (temporal split only)\n",
        "   - Sequence length: 10-50 time steps\n",
        "   - Prediction horizon: 1-10 time steps\n",
        "\n",
        "7. USE KERAS OR PYTORCH:\n",
        "   - Use framework's LSTM/GRU layers\n",
        "   - Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "   - Add positional encoding (custom implementation or built-in)\n",
        "   - Use standard training methods\n",
        "\n",
        "8. FILE SUBMISSION:\n",
        "   - Submit ONLY the .ipynb file\n",
        "   - NO zip files, NO separate data files, NO separate image files\n",
        "   - All code and outputs must be in the notebook\n",
        "   - Only one submission attempt allowed\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "f7883103",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7883103",
        "outputId": "0d63dc37-e02d-459f-89be-587ef8afc35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Import Required Libraries\n",
        "# ================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "\n",
        "# ================================\n",
        "# PyTorch Imports (LSTM & Transformer)\n",
        "# ================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ================================\n",
        "# Reproducibility\n",
        "# ================================\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# ================================\n",
        "# Device configuration\n",
        "# ================================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b889b4",
      "metadata": {
        "id": "17b889b4"
      },
      "source": [
        "Deep learning frameworks (choose Keras or PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "002c89de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "002c89de",
        "outputId": "91801dd6-36af-43ef-c650-9ab0d3f60853"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nPART 1: DATASET LOADING AND EXPLORATION (Informational)\\n================================================================================\\n\\nInstructions:\\n1. Choose ONE dataset from the allowed list\\n2. Load and explore the time series data\\n3. Fill in ALL required metadata fields below\\n4. Provide justification for your primary metric choice\\n\\nALLOWED DATASETS:\\n- Stock Prices (daily/hourly closing prices)\\n- Weather Data (temperature, humidity, pressure)\\n- Energy Consumption (electricity/power usage)\\n- Sensor Data (IoT sensor readings)\\n- Custom time series (with approval)\\n\\nREQUIRED OUTPUT:\\n- Print all metadata fields\\n- Time series plots\\n- Stationarity analysis\\n- Train/test split visualization\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 1: DATASET LOADING AND EXPLORATION (Informational)\n",
        "================================================================================\n",
        "\n",
        "Instructions:\n",
        "1. Choose ONE dataset from the allowed list\n",
        "2. Load and explore the time series data\n",
        "3. Fill in ALL required metadata fields below\n",
        "4. Provide justification for your primary metric choice\n",
        "\n",
        "ALLOWED DATASETS:\n",
        "- Stock Prices (daily/hourly closing prices)\n",
        "- Weather Data (temperature, humidity, pressure)\n",
        "- Energy Consumption (electricity/power usage)\n",
        "- Sensor Data (IoT sensor readings)\n",
        "- Custom time series (with approval)\n",
        "\n",
        "REQUIRED OUTPUT:\n",
        "- Print all metadata fields\n",
        "- Time series plots\n",
        "- Stationarity analysis\n",
        "- Train/test split visualization\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c8fef3",
      "metadata": {
        "id": "83c8fef3"
      },
      "source": [
        "1.1 Dataset Selection and Loading\n",
        "TODO: Load your chosen time series dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download dataset\n",
        "dataset_path = kagglehub.dataset_download(\"bhanupratapbiswas/weather-data\")\n",
        "print(\"Dataset downloaded to:\", dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx6Zqa0TSd43",
        "outputId": "0519b3a3-3bfa-4294-baa7-0f4d3a04c902"
      },
      "id": "tx6Zqa0TSd43",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'weather-data' dataset.\n",
            "Dataset downloaded to: /kaggle/input/weather-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "e2b9ac13",
      "metadata": {
        "id": "e2b9ac13"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# REQUIRED: Fill in these metadata fields\n",
        "# =============================================================================\n",
        "dataset_name = \"Weather Data Dataset\"\n",
        "dataset_source = \"Kaggle: bhanupratapbiswas/weather-data\"\n",
        "# NOTE: Exact number may change slightly after dropping NaNs\n",
        "n_samples = 8784          # hourly data for 1 year (non-leap year)\n",
        "n_features = 10           # 6 weather vars + hour_sin + hour_cos + dow_sin + dow_cos\n",
        "sequence_length = 30      # Lookback window (30 hours)\n",
        "prediction_horizon = 1    # Forecast 1 step ahead\n",
        "problem_type = \"time_series_forecasting\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "6ae533b6",
      "metadata": {
        "id": "6ae533b6"
      },
      "outputs": [],
      "source": [
        "primary_metric = \"RMSE\"\n",
        "\n",
        "metric_justification = \"\"\"\n",
        "Root Mean Squared Error (RMSE) is selected because it penalizes larger errors\n",
        "more heavily, making it suitable for evaluating forecasting models where\n",
        "large deviations are undesirable in weather prediction tasks.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "2fef57a2",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fef57a2",
        "outputId": "98f21270-7d9e-4cd2-aa0f-a4a197afb5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DATASET INFORMATION\n",
            "======================================================================\n",
            "Dataset: Weather Data Dataset\n",
            "Source: Kaggle: bhanupratapbiswas/weather-data\n",
            "Total Samples: 8784\n",
            "Number of Features: 10\n",
            "Sequence Length: 30\n",
            "Prediction Horizon: 1\n",
            "Primary Metric: RMSE\n",
            "Metric Justification: \n",
            "Root Mean Squared Error (RMSE) is selected because it penalizes larger errors\n",
            "more heavily, making it suitable for evaluating forecasting models where\n",
            "large deviations are undesirable in weather prediction tasks.\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Source: {dataset_source}\")\n",
        "print(f\"Total Samples: {n_samples}\")\n",
        "print(f\"Number of Features: {n_features}\")\n",
        "print(f\"Sequence Length: {sequence_length}\")\n",
        "print(f\"Prediction Horizon: {prediction_horizon}\")\n",
        "print(f\"Primary Metric: {primary_metric}\")\n",
        "print(f\"Metric Justification: {metric_justification}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Load Weather Dataset (Kaggle / Colab / Local compatible)\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Locate CSV file\n",
        "# --------------------------------------------------\n",
        "csv_files = glob.glob(os.path.join(dataset_path, \"*.csv\"))\n",
        "assert len(csv_files) > 0, \"No CSV files found in dataset directory!\"\n",
        "\n",
        "csv_path = csv_files[0]\n",
        "print(\"Using file:\", csv_path)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Load dataset\n",
        "# --------------------------------------------------\n",
        "df = pd.read_csv(csv_path).copy()\n",
        "\n",
        "print(\"Raw dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Parse datetime column\n",
        "# --------------------------------------------------\n",
        "if \"Formatted Date\" in df.columns:\n",
        "    df[\"Formatted Date\"] = pd.to_datetime(df[\"Formatted Date\"], utc=True)\n",
        "    df = df.sort_values(\"Formatted Date\")\n",
        "    df.set_index(\"Formatted Date\", inplace=True)\n",
        "\n",
        "elif \"Date/Time\" in df.columns:\n",
        "    df[\"Date/Time\"] = pd.to_datetime(df[\"Date/Time\"])\n",
        "    df = df.sort_values(\"Date/Time\")\n",
        "    df.set_index(\"Date/Time\", inplace=True)\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"No recognizable datetime column found!\")\n",
        "\n",
        "print(\"After datetime parsing:\", df.shape)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Time-based feature engineering (CYCLIC ENCODING)\n",
        "# --------------------------------------------------\n",
        "# Hour of day\n",
        "df[\"hour\"] = df.index.hour\n",
        "df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
        "df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
        "\n",
        "# Day of year (cyclic)\n",
        "days_in_year = 366 if df.index.is_leap_year.any() else 365\n",
        "df[\"doy_sin\"] = np.sin(2 * np.pi * df[\"dayofyear\"] / days_in_year)\n",
        "df[\"doy_cos\"] = np.cos(2 * np.pi * df[\"dayofyear\"] / days_in_year)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Select numeric features for forecasting\n",
        "# --------------------------------------------------\n",
        "FEATURE_COLUMNS = [\n",
        "    \"Temp_C\",               # Target\n",
        "    \"Rel Hum_%\",            # Humidity\n",
        "    \"Wind Speed_km/h\",      # Wind speed\n",
        "    \"Visibility_km\",        # Visibility\n",
        "    \"Press_kPa\",            # Pressure\n",
        "    \"hour_sin\",             # Cyclic hour\n",
        "    \"hour_cos\",\n",
        "    \"doy_sin\",              # Cyclic day of year\n",
        "    \"doy_cos\"\n",
        "]\n",
        "\n",
        "# Keep only required columns\n",
        "data = df[FEATURE_COLUMNS].copy()\n",
        "\n",
        "# Handle missing values (time-series safe)\n",
        "data = data.ffill().bfill()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Update metadata (AUTOGRADER-CRITICAL)\n",
        "# --------------------------------------------------\n",
        "n_samples = data.shape[0]\n",
        "n_features = data.shape[1]   # MUST be 9\n",
        "\n",
        "print(\"Selected data shape:\", data.shape)\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\n================ PREPROCESSING CHECK =================\")\n",
        "print(\"Missing values per column:\\n\", data.isna().sum())\n",
        "print(\"Final data shape:\", data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "yAEu7x2zhrrs",
        "outputId": "68641dbf-0752-418e-d4c5-6ca23cf285df"
      },
      "id": "yAEu7x2zhrrs",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using file: /kaggle/input/weather-data/Weather Data.csv\n",
            "Raw dataset shape: (8784, 8)\n",
            "       Date/Time  Temp_C  Dew Point Temp_C  Rel Hum_%  Wind Speed_km/h  \\\n",
            "0  1/1/2012 0:00    -1.8              -3.9         86                4   \n",
            "1  1/1/2012 1:00    -1.8              -3.7         87                4   \n",
            "2  1/1/2012 2:00    -1.8              -3.4         89                7   \n",
            "3  1/1/2012 3:00    -1.5              -3.2         88                6   \n",
            "4  1/1/2012 4:00    -1.5              -3.3         88                7   \n",
            "\n",
            "   Visibility_km  Press_kPa               Weather  \n",
            "0            8.0     101.24                   Fog  \n",
            "1            8.0     101.24                   Fog  \n",
            "2            4.0     101.26  Freezing Drizzle,Fog  \n",
            "3            4.0     101.27  Freezing Drizzle,Fog  \n",
            "4            4.8     101.23                   Fog  \n",
            "After datetime parsing: (8784, 7)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'dayofyear'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'dayofyear'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3133892555.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Day of year (cyclic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mdays_in_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m366\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leap_year\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m365\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"doy_sin\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dayofyear\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdays_in_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"doy_cos\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dayofyear\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdays_in_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'dayofyear'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd8ddfa",
      "metadata": {
        "id": "dfd8ddfa"
      },
      "source": [
        "1.2 Time Series Exploration\n",
        "TODO: Plot time series data\n",
        "TODO: Check for trends, seasonality\n",
        "TODO: Perform stationarity tests (optional but recommended)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 1.2 Time Series Exploration\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. Plot time series data\n",
        "# --------------------------------------------------\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data.index, data[\"Temp_C\"], label=\"Temperature (C)\")\n",
        "plt.title(\"Time Series of Temperature\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Temperature (C)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Optional: plot other features\n",
        "data[[\"Rel Hum_%\", \"Wind Speed_km/h\", \"Visibility_km\", \"Press_kPa\"]].plot(\n",
        "    figsize=(14, 6), subplots=True, layout=(2,2), title=\"Weather Features\"\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. Decompose time series (trend, seasonality, residual)\n",
        "# --------------------------------------------------\n",
        "# Using additive model (suitable if seasonal variations are roughly constant)\n",
        "decompose_result = seasonal_decompose(data[\"Temp_C\"], model='additive', period=24)  # period=24 if hourly, adjust if daily\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "decompose_result.plot()\n",
        "plt.suptitle(\"Seasonal Decomposition of Temperature\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3. Stationarity test (Augmented Dickey-Fuller)\n",
        "# --------------------------------------------------\n",
        "adf_result = adfuller(data[\"Temp_C\"].dropna())\n",
        "print(\"ADF Statistic:\", adf_result[0])\n",
        "print(\"p-value:\", adf_result[1])\n",
        "for key, value in adf_result[4].items():\n",
        "    print(\"Critical Value ({}): {:.3f}\".format(key, value))\n",
        "\n",
        "if adf_result[1] < 0.05:\n",
        "    print(\"=> The series is likely stationary.\")\n",
        "else:\n",
        "    print(\"=> The series is likely non-stationary, differencing may be needed.\")\n"
      ],
      "metadata": {
        "id": "MhZVJGvDhwy6"
      },
      "id": "MhZVJGvDhwy6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d498e253",
      "metadata": {
        "id": "d498e253"
      },
      "source": [
        "1.3 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6efdacdc",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "6efdacdc"
      },
      "outputs": [],
      "source": [
        "def preprocess_timeseries(data):\n",
        "    \"\"\"\n",
        "    Preprocess time series data\n",
        "\n",
        "    Args:\n",
        "        data: raw time series data (pandas DataFrame)\n",
        "\n",
        "    Returns:\n",
        "        preprocessed data (numpy array), scaler\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy as np\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Handle missing values (time-series safe)\n",
        "    # --------------------------------------------------\n",
        "    data = data.copy()\n",
        "    data = data.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Normalize data\n",
        "    # --------------------------------------------------\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(data.values)\n",
        "\n",
        "    return scaled_data, scaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d888128f",
      "metadata": {
        "id": "d888128f"
      },
      "source": [
        "TODO: Preprocess data\n",
        "TODO: Create sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0da181",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ea0da181"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 1.4 Sequence Generation\n",
        "# =============================================================================\n",
        "\n",
        "def create_sequences(data, seq_length, pred_horizon):\n",
        "    \"\"\"\n",
        "    Create sequences for time series prediction\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): normalized data (n_samples, n_features)\n",
        "        seq_length (int): lookback window\n",
        "        pred_horizon (int): forecast horizon\n",
        "\n",
        "    Returns:\n",
        "        X (np.ndarray): (samples, seq_length, n_features)\n",
        "        y (np.ndarray): (samples, 1) or (samples, horizon)\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, np.ndarray), \"Data must be NumPy array\"\n",
        "    assert data.ndim == 2, \"Data must be 2D\"\n",
        "    assert seq_length > 0 and pred_horizon > 0\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(data) - seq_length - pred_horizon + 1):\n",
        "\n",
        "        # Input window\n",
        "        X.append(data[i : i + seq_length])\n",
        "\n",
        "        # Target: Temperature only (first column)\n",
        "        y.append(\n",
        "            data[i + seq_length : i + seq_length + pred_horizon, 0]\n",
        "        )\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.float32)\n",
        "\n",
        "    # Shape normalization for single-step forecasting\n",
        "    if pred_horizon == 1:\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "    print(\"\\n================ SEQUENCE GENERATION =================\")\n",
        "    print(\"X shape:\", X.shape)\n",
        "    print(\"y shape:\", y.shape)\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Feature Selection\n",
        "# =============================================================================\n",
        "\n",
        "feature_cols = [\n",
        "    \"Temp_C\",            # Target (kept for scaling + sequence creation)\n",
        "    \"Rel Hum_%\",\n",
        "    \"Wind Speed_km/h\",\n",
        "    \"Visibility_km\",\n",
        "    \"Press_kPa\",\n",
        "    \"hour_sin\",\n",
        "    \"hour_cos\",\n",
        "    \"doy_sin\",\n",
        "    \"doy_cos\"\n",
        "]\n",
        "\n",
        "scaled_data, scaler = preprocess_timeseries(\n",
        "    data=df,\n",
        "    feature_cols=feature_cols\n",
        ")\n",
        "\n",
        "X, y = create_sequences(\n",
        "    data=scaled_data,\n",
        "    seq_length=sequence_length,\n",
        "    pred_horizon=prediction_horizon\n",
        ")\n"
      ],
      "metadata": {
        "id": "1ri3PQoYl6OA"
      },
      "id": "1ri3PQoYl6OA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "725e3489",
      "metadata": {
        "id": "725e3489"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 1.5 Train / Test Split (Temporal – NO SHUFFLING)\n",
        "# =============================================================================\n",
        "\n",
        "train_test_ratio = 0.9  # 90% train, 10% test\n",
        "\n",
        "n_total = X.shape[0]\n",
        "train_samples = int(n_total * train_test_ratio)\n",
        "test_samples = n_total - train_samples\n",
        "\n",
        "# Temporal split (VERY IMPORTANT)\n",
        "X_train = X[:train_samples]\n",
        "y_train = y[:train_samples]\n",
        "\n",
        "X_test = X[train_samples:]\n",
        "y_test = y[train_samples:]\n",
        "\n",
        "# Sanity checks\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa1ee1f",
      "metadata": {
        "id": "daa1ee1f"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nTrain/Test Split: {train_test_ratio}\")\n",
        "print(f\"Training Samples: {train_samples}\")\n",
        "print(f\"Test Samples: {test_samples}\")\n",
        "print(\"⚠️  IMPORTANT: Temporal split used (NO shuffling)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef664ae",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "9ef664ae"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 2: LSTM/GRU IMPLEMENTATION (5 MARKS)\n",
        "================================================================================\n",
        "\n",
        "REQUIREMENTS:\n",
        "- Build LSTM OR GRU using Keras/PyTorch layers\n",
        "- Architecture must include:\n",
        "  * At least 2 stacked recurrent layers\n",
        "  * Output layer for prediction\n",
        "- Use model.compile() and model.fit() (Keras) OR standard PyTorch training\n",
        "- Track initial_loss and final_loss\n",
        "\n",
        "GRADING:\n",
        "- LSTM/GRU architecture with stacked layers: 2 marks\n",
        "- Model properly compiled/configured: 1 mark\n",
        "- Training completed with loss tracking: 1 mark\n",
        "- All metrics calculated correctly: 1 mark\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b349fea",
      "metadata": {
        "id": "0b349fea"
      },
      "source": [
        "2.1 LSTM/GRU Architecture Design\n",
        "TODO: Choose LSTM or GRU\n",
        "TODO: Design architecture with stacked layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87daaa54",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "87daaa54"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 2.1 LSTM/GRU Architecture Design\n",
        "# =============================================================================\n",
        "# - Model Type: LSTM or GRU\n",
        "# - Stacked recurrent layers (n_layers >= 2)\n",
        "# - Output layer for forecasting\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_rnn_model(model_type, input_shape, hidden_units, n_layers, output_size):\n",
        "    \"\"\"\n",
        "    Build LSTM or GRU model\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Assertions (ENFORCED by assignment)\n",
        "    # ------------------------------------------------------------------\n",
        "    assert model_type in [\"LSTM\", \"GRU\"], \"model_type must be 'LSTM' or 'GRU'\"\n",
        "    assert n_layers >= 2, \"n_layers must be at least 2 (stacked layers REQUIRED)\"\n",
        "\n",
        "    _, n_features = input_shape\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # RNN Model Definition\n",
        "    # ------------------------------------------------------------------\n",
        "    class RNNModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(RNNModel, self).__init__()\n",
        "\n",
        "            if model_type == \"LSTM\":\n",
        "                self.rnn = nn.LSTM(\n",
        "                    input_size=n_features,\n",
        "                    hidden_size=hidden_units,\n",
        "                    num_layers=n_layers,\n",
        "                    batch_first=True\n",
        "                )\n",
        "            else:  # GRU\n",
        "                self.rnn = nn.GRU(\n",
        "                    input_size=n_features,\n",
        "                    hidden_size=hidden_units,\n",
        "                    num_layers=n_layers,\n",
        "                    batch_first=True\n",
        "                )\n",
        "\n",
        "            # Output layer\n",
        "            self.fc = nn.Linear(hidden_units, output_size)\n",
        "\n",
        "        def forward(self, x):\n",
        "            out, _ = self.rnn(x)\n",
        "            out = out[:, -1, :]   # last time step\n",
        "            out = self.fc(out)\n",
        "            return out\n",
        "\n",
        "    model = RNNModel()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9e46a1",
      "metadata": {
        "id": "8e9e46a1"
      },
      "source": [
        "TODO: Create RNN model\n",
        "rnn_model = build_rnn_model('LSTM', (sequence_length, n_features), 64, 2, prediction_horizon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Create RNN Model\n",
        "# =============================================================================\n",
        "\n",
        "rnn_model = build_rnn_model(\n",
        "    model_type=\"LSTM\",\n",
        "    input_shape=(sequence_length, n_features),\n",
        "    hidden_units=64,\n",
        "    n_layers=2,\n",
        "    output_size=prediction_horizon\n",
        ")\n",
        "\n",
        "rnn_model = rnn_model.to(device)\n",
        "\n",
        "print(rnn_model)\n"
      ],
      "metadata": {
        "id": "gRrKk5l3oce9"
      },
      "id": "gRrKk5l3oce9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "98bf8c67",
      "metadata": {
        "id": "98bf8c67"
      },
      "source": [
        "TODO: Compile model\n",
        "For Keras: model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "For PyTorch: define optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(\n",
        "    rnn_model.parameters(),\n",
        "    lr=0.001\n",
        ")\n"
      ],
      "metadata": {
        "id": "YljdHAXIo_j3"
      },
      "id": "YljdHAXIo_j3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "44df1c4b",
      "metadata": {
        "id": "44df1c4b"
      },
      "source": [
        "2.2 Train RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caebf1f4",
      "metadata": {
        "id": "caebf1f4"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RNN MODEL TRAINING\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e65f6c",
      "metadata": {
        "id": "b8e65f6c"
      },
      "outputs": [],
      "source": [
        "# Track training time\n",
        "rnn_start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d32150",
      "metadata": {
        "id": "f4d32150"
      },
      "source": [
        "TODO: Train your model\n",
        "For Keras: history = rnn_model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "For PyTorch: write training loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------------------------------\n",
        "# Move model to device\n",
        "# -------------------------------\n",
        "rnn_model = rnn_model.to(device)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "epochs = 25\n",
        "loss_history = []\n",
        "\n",
        "rnn_model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = rnn_model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # RNN gradient safety\n",
        "        torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    loss_history.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n"
      ],
      "metadata": {
        "id": "5MO7ioecqIHz"
      },
      "id": "5MO7ioecqIHz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b093f3f",
      "metadata": {
        "id": "3b093f3f"
      },
      "outputs": [],
      "source": [
        "rnn_training_time = time.time() - rnn_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ade3a8",
      "metadata": {
        "id": "02ade3a8"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Track initial and final loss\n",
        "rnn_initial_loss = loss_history[0]\n",
        "rnn_final_loss   = loss_history[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49554c49",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "49554c49"
      },
      "outputs": [],
      "source": [
        "print(f\"Training completed in {rnn_training_time:.2f} seconds\")\n",
        "print(f\"Initial Loss: {rnn_initial_loss:.4f}\")\n",
        "print(f\"Final Loss: {rnn_final_loss:.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Alias for later comparison plots\n",
        "rnn_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792870c6",
      "metadata": {
        "id": "792870c6"
      },
      "source": [
        "2.3 Evaluate RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f459c549",
      "metadata": {
        "id": "f459c549"
      },
      "source": [
        "TODO: Make predictions on test set\n",
        "TODO: Inverse transform if data was normalized\n",
        "TODO: Calculate all 4 required metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2.3 Evaluate RNN Model\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# -------------------------------\n",
        "# Model inference\n",
        "# -------------------------------\n",
        "rnn_model.eval()\n",
        "\n",
        "y_preds = []\n",
        "y_trues = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        preds = rnn_model(xb)\n",
        "\n",
        "        y_preds.append(preds.cpu().numpy())\n",
        "        y_trues.append(yb.cpu().numpy())\n",
        "\n",
        "y_pred_scaled = np.vstack(y_preds)\n",
        "y_true_scaled = np.vstack(y_trues)\n",
        "\n",
        "\n",
        "if prediction_horizon > 1:\n",
        "    y_pred_scaled = y_pred_scaled[:, 0]\n",
        "    y_true_scaled = y_true_scaled[:, 0]\n",
        "else:\n",
        "    y_pred_scaled = y_pred_scaled.reshape(-1)\n",
        "    y_true_scaled = y_true_scaled.reshape(-1)\n",
        "\n",
        "n_features = scaled_data.shape[1]\n",
        "\n",
        "y_pred_dummy = np.zeros((len(y_pred_scaled), n_features))\n",
        "y_true_dummy = np.zeros((len(y_true_scaled), n_features))\n",
        "\n",
        "y_pred_dummy[:, 0] = y_pred_scaled\n",
        "y_true_dummy[:, 0] = y_true_scaled\n",
        "\n",
        "y_pred = scaler.inverse_transform(y_pred_dummy)[:, 0]\n",
        "y_true = scaler.inverse_transform(y_true_dummy)[:, 0]\n",
        "\n",
        "rnn_y_true = y_true\n",
        "rnn_y_pred = y_pred\n"
      ],
      "metadata": {
        "id": "5C09S9intnN0"
      },
      "id": "5C09S9intnN0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1678f898",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "1678f898"
      },
      "outputs": [],
      "source": [
        "def calculate_mape(y_true, y_pred):\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43dcb302",
      "metadata": {
        "id": "43dcb302"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Calculate all 4 metrics\n",
        "rnn_mae  = mean_absolute_error(y_true, y_pred)\n",
        "rnn_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "rnn_mape = calculate_mape(y_true, y_pred)\n",
        "rnn_r2   = r2_score(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7eb4ceb",
      "metadata": {
        "id": "d7eb4ceb"
      },
      "outputs": [],
      "source": [
        "print(\"\\nRNN Model Performance:\")\n",
        "print(f\"MAE:   {rnn_mae:.4f}\")\n",
        "print(f\"RMSE:  {rnn_rmse:.4f}\")\n",
        "print(f\"MAPE:  {rnn_mape:.4f}%\")\n",
        "print(f\"R² Score: {rnn_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5be5a3",
      "metadata": {
        "id": "8a5be5a3"
      },
      "source": [
        "2.4 Visualize RNN Results\n",
        "TODO: Plot training loss curve\n",
        "TODO: Plot actual vs predicted values\n",
        "TODO: Plot residuals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2.4 Visualize RNN Results\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot training loss curve\n",
        "# ------------------------------------------------------------------\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(loss_history, marker=\"o\", linewidth=2)\n",
        "plt.title(\"RNN Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot Actual vs Predicted values\n",
        "# ------------------------------------------------------------------\n",
        "n_plot = 500\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(y_true[:n_plot], label=\"Actual\", linewidth=2)\n",
        "plt.plot(y_pred[:n_plot], label=\"Predicted\", alpha=0.8)\n",
        "plt.title(\"Actual vs Predicted Temperature (RNN)\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Temperature (°C)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot Residuals\n",
        "# ------------------------------------------------------------------\n",
        "residuals = y_true[:n_plot] - y_pred[:n_plot]\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(residuals, color=\"red\", alpha=0.8)\n",
        "plt.axhline(0, linestyle=\"--\", color=\"black\")\n",
        "plt.title(\"Residuals (Actual − Predicted)\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Prediction Error (°C)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7q-UQRbFv3eZ"
      },
      "id": "7q-UQRbFv3eZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411d84b3",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "411d84b3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 3: TRANSFORMER IMPLEMENTATION (5 MARKS)\n",
        "================================================================================\n",
        "\n",
        "REQUIREMENTS:\n",
        "- Build Transformer encoder using Keras/PyTorch layers\n",
        "- MUST add positional encoding to input:\n",
        "  * Custom sinusoidal implementation OR\n",
        "  * Use built-in positional encoding (if framework provides)\n",
        "- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "- Use standard training methods\n",
        "- Track initial_loss and final_loss\n",
        "\n",
        "PROHIBITED:\n",
        "- Using pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
        "- Skipping positional encoding entirely\n",
        "\n",
        "GRADING:\n",
        "- Positional encoding added: 1 mark\n",
        "- Transformer architecture properly configured: 2 marks\n",
        "- Training completed with loss tracking: 1 mark\n",
        "- All metrics calculated correctly: 1 mark\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044873c0",
      "metadata": {
        "id": "044873c0"
      },
      "source": [
        "3.1 Positional Encoding Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335f5f6b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "335f5f6b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        return x + self.pe[:, :x.size(1), :]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83741f21",
      "metadata": {
        "id": "83741f21"
      },
      "source": [
        "3.2 Transformer Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1c4463",
      "metadata": {
        "id": "2f1c4463"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, n_features, d_model, n_heads, n_layers, d_ff, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_projection = nn.Linear(n_features, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, n_features)\n",
        "        x = self.input_projection(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f72750",
      "metadata": {
        "id": "06f72750"
      },
      "source": [
        "3.3 Build Your Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db4f26b",
      "metadata": {
        "id": "2db4f26b"
      },
      "source": [
        "TODO: Create Transformer model using PyTorch or Keras\n",
        "Example for PyTorch:\n",
        "transformer_model = TransformerModel(n_features, d_model=64, n_heads=4, n_layers=2, d_ff=256, output_size=prediction_horizon)\n",
        "Example for Keras:\n",
        "transformer_model = build_transformer_model(sequence_length, n_features, d_model=64, n_heads=4, n_layers=2, d_ff=256, output_size=prediction_horizon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3.3 Build Transformer Model\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create Transformer model\n",
        "transformer_model = TransformerModel(\n",
        "    n_features=n_features,\n",
        "    d_model=64,\n",
        "    n_heads=4,\n",
        "    n_layers=2,\n",
        "    d_ff=256,\n",
        "    output_size=prediction_horizon\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "transformer_model = transformer_model.to(device)\n"
      ],
      "metadata": {
        "id": "dp8YT9UCza_u"
      },
      "id": "dp8YT9UCza_u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5f1bc8c9",
      "metadata": {
        "id": "5f1bc8c9"
      },
      "source": [
        "TODO: Define optimizer and loss\n",
        "For PyTorch: optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001); criterion = nn.MSELoss()\n",
        "For Keras: model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "For PyTorch: define optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n"
      ],
      "metadata": {
        "id": "Ih2bNBLAzcAO"
      },
      "id": "Ih2bNBLAzcAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c00d6c9c",
      "metadata": {
        "id": "c00d6c9c"
      },
      "source": [
        "3.4 Train Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2325454b",
      "metadata": {
        "id": "2325454b"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRANSFORMER MODEL TRAINING\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ed1263",
      "metadata": {
        "id": "26ed1263"
      },
      "outputs": [],
      "source": [
        "# Track training time\n",
        "transformer_start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "846c1aab",
      "metadata": {
        "id": "846c1aab"
      },
      "source": [
        "TODO: Train your model\n",
        "For Keras: history = transformer_model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "For PyTorch: write training loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 25\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "transformer_loss_history = []\n",
        "\n",
        "transformer_model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = transformer_model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # 🔒 Safety: gradient clipping (good practice for transformers too)\n",
        "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    transformer_loss_history.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n"
      ],
      "metadata": {
        "id": "PshAi2aH1Kz1"
      },
      "id": "PshAi2aH1Kz1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9171c703",
      "metadata": {
        "id": "9171c703"
      },
      "outputs": [],
      "source": [
        "transformer_training_time = time.time() - transformer_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9e89eb",
      "metadata": {
        "id": "3f9e89eb"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Track initial and final loss\n",
        "transformer_initial_loss = transformer_loss_history[0]\n",
        "transformer_final_loss   = transformer_loss_history[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddfc7240",
      "metadata": {
        "id": "ddfc7240"
      },
      "outputs": [],
      "source": [
        "print(f\"Training completed in {transformer_training_time:.2f} seconds\")\n",
        "print(f\"Initial Loss: {transformer_initial_loss:.4f}\")\n",
        "print(f\"Final Loss: {transformer_final_loss:.4f}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3707b1e",
      "metadata": {
        "id": "f3707b1e"
      },
      "source": [
        "3.5 Evaluate Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6e3fa1",
      "metadata": {
        "id": "6a6e3fa1"
      },
      "source": [
        "TODO: Make predictions on test set\n",
        "TODO: Inverse transform if data was normalized\n",
        "TODO: Calculate all 4 required metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3.5 Evaluate Transformer Model\n",
        "# =============================================================================\n",
        "\n",
        "transformer_model.eval()\n",
        "\n",
        "y_preds = []\n",
        "y_trues = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        preds = transformer_model(xb)\n",
        "\n",
        "        y_preds.append(preds.cpu().numpy())\n",
        "        y_trues.append(yb.cpu().numpy())\n",
        "\n",
        "y_pred_scaled = np.vstack(y_preds)\n",
        "y_true_scaled = np.vstack(y_trues)\n",
        "\n",
        "if prediction_horizon > 1:\n",
        "    y_pred_scaled = y_pred_scaled[:, 0]\n",
        "    y_true_scaled = y_true_scaled[:, 0]\n",
        "else:\n",
        "    y_pred_scaled = y_pred_scaled.reshape(-1)\n",
        "    y_true_scaled = y_true_scaled.reshape(-1)\n",
        "\n",
        "n_features = scaled_data.shape[1]\n",
        "\n",
        "y_pred_dummy = np.zeros((len(y_pred_scaled), n_features))\n",
        "y_true_dummy = np.zeros((len(y_true_scaled), n_features))\n",
        "\n",
        "y_pred_dummy[:, 0] = y_pred_scaled\n",
        "y_true_dummy[:, 0] = y_true_scaled\n",
        "\n",
        "y_pred = scaler.inverse_transform(y_pred_dummy)[:, 0]\n",
        "y_true = scaler.inverse_transform(y_true_dummy)[:, 0]\n",
        "\n",
        "transformer_y_true = y_true\n",
        "transformer_y_pred = y_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "gTqWGZzT5XyP"
      },
      "id": "gTqWGZzT5XyP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e024b42c",
      "metadata": {
        "id": "e024b42c"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Calculate all 4 metrics\n",
        "transformer_mae  = mean_absolute_error(y_true, y_pred)\n",
        "transformer_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "transformer_mape = calculate_mape(y_true, y_pred)\n",
        "transformer_r2   = r2_score(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0126e84",
      "metadata": {
        "id": "c0126e84"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTransformer Model Performance:\")\n",
        "print(f\"MAE:   {transformer_mae:.4f}\")\n",
        "print(f\"RMSE:  {transformer_rmse:.4f}\")\n",
        "print(f\"MAPE:  {transformer_mape:.4f}%\")\n",
        "print(f\"R² Score: {transformer_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d3a7f0",
      "metadata": {
        "id": "c9d3a7f0"
      },
      "source": [
        "3.6 Visualize Transformer Results\n",
        "TODO: Plot training loss curve\n",
        "TODO: Plot actual vs predicted values\n",
        "TODO: Plot attention weights (optional but informative)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3.6 Visualize Transformer Results\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot training loss curve\n",
        "# ------------------------------------------------------------------\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(transformer_loss_history, marker=\"o\", linewidth=2)\n",
        "plt.title(\"Transformer Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot Actual vs Predicted values\n",
        "# ------------------------------------------------------------------\n",
        "n_plot = 500\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(y_true[:n_plot], label=\"Actual\", linewidth=2)\n",
        "plt.plot(y_pred[:n_plot], label=\"Predicted\", alpha=0.8)\n",
        "plt.title(\"Actual vs Predicted Temperature (Transformer)\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Temperature (°C)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mRNuak7-58nH"
      },
      "id": "mRNuak7-58nH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98256c4c",
      "metadata": {
        "id": "98256c4c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 4: MODEL COMPARISON AND VISUALIZATION (Informational)\n",
        "================================================================================\n",
        "\n",
        "Compare both models on:\n",
        "- Performance metrics\n",
        "- Training time\n",
        "- Model complexity\n",
        "- Convergence behavior\n",
        "- Ability to capture long-term dependencies\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e98c47",
      "metadata": {
        "id": "14e98c47"
      },
      "source": [
        "4.1 Metrics Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af1fbf3c",
      "metadata": {
        "id": "af1fbf3c"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0e5a47",
      "metadata": {
        "id": "dc0e5a47"
      },
      "outputs": [],
      "source": [
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['MAE', 'RMSE', 'MAPE (%)', 'R² Score', 'Training Time (s)', 'Parameters'],\n",
        "    'RNN (LSTM/GRU)': [\n",
        "        rnn_mae,\n",
        "        rnn_rmse,\n",
        "        rnn_mape,\n",
        "        rnn_r2,\n",
        "        rnn_training_time,\n",
        "        sum(p.numel() for p in rnn_model.parameters() if p.requires_grad)\n",
        "    ],\n",
        "    'Transformer': [\n",
        "        transformer_mae,\n",
        "        transformer_rmse,\n",
        "        transformer_mape,\n",
        "        transformer_r2,\n",
        "        transformer_training_time,\n",
        "        sum(p.numel() for p in transformer_model.parameters() if p.requires_grad)\n",
        "    ]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a860057",
      "metadata": {
        "id": "2a860057"
      },
      "outputs": [],
      "source": [
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ac551d",
      "metadata": {
        "id": "70ac551d"
      },
      "source": [
        "4.2 Visual Comparison\n",
        "TODO: Create bar plot comparing metrics\n",
        "TODO: Plot predictions comparison (both models vs actual)\n",
        "TODO: Plot training curves comparison"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = ['MAE', 'RMSE', 'MAPE (%)', 'R² Score']\n",
        "\n",
        "rnn_metrics = [\n",
        "    rnn_mae,\n",
        "    rnn_rmse,\n",
        "    rnn_mape,\n",
        "    rnn_r2\n",
        "]\n",
        "\n",
        "transformer_metrics = [\n",
        "    transformer_mae,\n",
        "    transformer_rmse,\n",
        "    transformer_mape,\n",
        "    transformer_r2\n",
        "]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(x - width/2, rnn_metrics, width, label='RNN')\n",
        "plt.bar(x + width/2, transformer_metrics, width, label='Transformer')\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(x, metrics)\n",
        "plt.legend()\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "n_plot = 500  # number of time steps to visualize\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(y_true[:n_plot], label='Actual', linewidth=2)\n",
        "plt.plot(rnn_y_pred[:n_plot], label='RNN Prediction', alpha=0.8)\n",
        "plt.plot(transformer_y_pred[:n_plot], label='Transformer Prediction', alpha=0.8)\n",
        "\n",
        "plt.title('Actual vs Predicted Temperature')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Temperature (°C)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(rnn_loss_history, label='RNN Training Loss', linewidth=2)\n",
        "plt.plot(transformer_loss_history, label='Transformer Training Loss', linewidth=2)\n",
        "\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tKeTdmpF7Kjv"
      },
      "id": "tKeTdmpF7Kjv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22896627",
      "metadata": {
        "id": "22896627"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 5: ANALYSIS (2 MARKS)\n",
        "================================================================================\n",
        "\n",
        "REQUIRED:\n",
        "- Write MAXIMUM 200 words (guideline - no marks deduction if exceeded)\n",
        "- Address key topics with depth\n",
        "\n",
        "GRADING (Quality-based):\n",
        "- Covers 5+ key topics with deep understanding: 2 marks\n",
        "- Covers 3-4 key topics with good understanding: 1 mark\n",
        "- Covers <3 key topics or superficial: 0 marks\n",
        "\n",
        "Key Topics:\n",
        "1. Performance comparison with specific metrics\n",
        "2. RNN vs Transformer architecture advantages\n",
        "3. Impact of attention mechanism vs recurrent connections\n",
        "4. Long-term dependency handling comparison\n",
        "5. Computational cost comparison\n",
        "6. Convergence behavior differences\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05be677",
      "metadata": {
        "id": "d05be677"
      },
      "outputs": [],
      "source": [
        "analysis_text = \"\"\"\n",
        "TODO: Write your analysis here (maximum 200 words guideline)\n",
        "\n",
        "Address:\n",
        "1. Which model performed better and by how much?\n",
        "   [Compare specific metrics]\n",
        "\n",
        "2. RNN vs Transformer architecture advantages?\n",
        "   [Discuss sequential processing vs parallel processing]\n",
        "\n",
        "3. Impact of attention mechanism?\n",
        "   [Discuss how attention captures dependencies]\n",
        "\n",
        "4. Long-term dependency handling?\n",
        "   [Compare vanishing gradients vs attention]\n",
        "\n",
        "5. Computational cost comparison?\n",
        "   [Compare training time, parameters]\n",
        "\n",
        "6. Convergence behavior?\n",
        "   [Discuss training stability, loss curves]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05df3e69",
      "metadata": {
        "id": "05df3e69"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Print analysis with word count\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(analysis_text)\n",
        "print(\"=\"*70)\n",
        "print(f\"Analysis word count: {len(analysis_text.split())} words\")\n",
        "if len(analysis_text.split()) > 200:\n",
        "    print(\"⚠️  Warning: Analysis exceeds 200 words (guideline)\")\n",
        "else:\n",
        "    print(\"✓ Analysis within word count guideline\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f2ce90",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "a4f2ce90"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 6: ASSIGNMENT RESULTS SUMMARY (REQUIRED FOR AUTO-GRADING)\n",
        "================================================================================\n",
        "\n",
        "DO NOT MODIFY THE STRUCTURE BELOW\n",
        "This JSON output is used by the auto-grader\n",
        "Ensure all field names are EXACT\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c800bf2e",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "c800bf2e"
      },
      "outputs": [],
      "source": [
        "def get_assignment_results():\n",
        "    \"\"\"\n",
        "    Generate complete assignment results in required format\n",
        "\n",
        "    Returns:\n",
        "        dict: Complete results with all required fields\n",
        "    \"\"\"\n",
        "\n",
        "    framework_used = \"keras\"  # TODO: Change to \"pytorch\" if using PyTorch\n",
        "    rnn_model_type = \"LSTM\"  # TODO: Change to \"GRU\" if using GRU\n",
        "\n",
        "    results = {\n",
        "        # Dataset Information\n",
        "        'dataset_name': dataset_name,\n",
        "        'dataset_source': dataset_source,\n",
        "        'n_samples': n_samples,\n",
        "        'n_features': n_features,\n",
        "        'sequence_length': sequence_length,\n",
        "        'prediction_horizon': prediction_horizon,\n",
        "        'problem_type': problem_type,\n",
        "        'primary_metric': primary_metric,\n",
        "        'metric_justification': metric_justification,\n",
        "        'train_samples': train_samples,\n",
        "        'test_samples': test_samples,\n",
        "        'train_test_ratio': train_test_ratio,\n",
        "\n",
        "        # RNN Model Results\n",
        "        'rnn_model': {\n",
        "            'framework': framework_used,\n",
        "            'model_type': rnn_model_type,\n",
        "            'architecture': {\n",
        "                'n_layers': 0,  # TODO: Number of stacked layers\n",
        "                'hidden_units': 0,  # TODO: Hidden units per layer\n",
        "                'total_parameters': 0  # TODO: Calculate total parameters\n",
        "            },\n",
        "            'training_config': {\n",
        "                'learning_rate': 0.001,  # TODO: Your actual learning rate\n",
        "                'n_epochs': 50,  # TODO: Your actual epochs\n",
        "                'batch_size': 32,  # TODO: Your actual batch size\n",
        "                'optimizer': 'Adam',  # TODO: Your actual optimizer\n",
        "                'loss_function': 'MSE'  # TODO: Your actual loss\n",
        "            },\n",
        "            'initial_loss': rnn_initial_loss,\n",
        "            'final_loss': rnn_final_loss,\n",
        "            'training_time_seconds': rnn_training_time,\n",
        "            'mae': rnn_mae,\n",
        "            'rmse': rnn_rmse,\n",
        "            'mape': rnn_mape,\n",
        "            'r2_score': rnn_r2\n",
        "        },\n",
        "\n",
        "        # Transformer Model Results\n",
        "        'transformer_model': {\n",
        "            'framework': framework_used,\n",
        "            'architecture': {\n",
        "                'n_layers': 0,  # TODO: Number of transformer layers\n",
        "                'n_heads': 0,  # TODO: Number of attention heads\n",
        "                'd_model': 0,  # TODO: Model dimension\n",
        "                'd_ff': 0,  # TODO: Feed-forward dimension\n",
        "                'has_positional_encoding': True,  # MUST be True\n",
        "                'has_attention': True,  # MUST be True\n",
        "                'total_parameters': 0  # TODO: Calculate total parameters\n",
        "            },\n",
        "            'training_config': {\n",
        "                'learning_rate': 0.001,  # TODO: Your actual learning rate\n",
        "                'n_epochs': 50,  # TODO: Your actual epochs\n",
        "                'batch_size': 32,  # TODO: Your actual batch size\n",
        "                'optimizer': 'Adam',  # TODO: Your actual optimizer\n",
        "                'loss_function': 'MSE'  # TODO: Your actual loss\n",
        "            },\n",
        "            'initial_loss': transformer_initial_loss,\n",
        "            'final_loss': transformer_final_loss,\n",
        "            'training_time_seconds': transformer_training_time,\n",
        "            'mae': transformer_mae,\n",
        "            'rmse': transformer_rmse,\n",
        "            'mape': transformer_mape,\n",
        "            'r2_score': transformer_r2\n",
        "        },\n",
        "\n",
        "        # Analysis\n",
        "        'analysis': analysis_text,\n",
        "        'analysis_word_count': len(analysis_text.split()),\n",
        "\n",
        "        # Training Success Indicators\n",
        "        'rnn_loss_decreased': rnn_final_loss < rnn_initial_loss if rnn_initial_loss and rnn_final_loss else False,\n",
        "        'transformer_loss_decreased': transformer_final_loss < transformer_initial_loss if transformer_initial_loss and transformer_final_loss else False,\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694a274d",
      "metadata": {
        "id": "694a274d"
      },
      "outputs": [],
      "source": [
        "# Generate and print results\n",
        "try:\n",
        "    assignment_results = get_assignment_results()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ASSIGNMENT RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(json.dumps(assignment_results, indent=2))\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce921018",
      "metadata": {
        "id": "ce921018"
      },
      "outputs": [],
      "source": [
        "except Exception as e:\n",
        "    print(f\"\\n⚠️  ERROR generating results: {str(e)}\")\n",
        "    print(\"Please ensure all variables are properly defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f84f5c",
      "metadata": {
        "id": "60f84f5c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "FINAL CHECKLIST - VERIFY BEFORE SUBMISSION\n",
        "================================================================================\n",
        "\n",
        "□ Student information filled at the top (BITS ID, Name, Email)\n",
        "□ Filename is <BITS_ID>_rnn_assignment.ipynb\n",
        "□ All cells executed (Kernel → Restart & Run All)\n",
        "□ All outputs visible\n",
        "□ LSTM/GRU implemented with stacked layers\n",
        "□ Positional encoding implemented (sinusoidal)\n",
        "□ Multi-head attention implemented (Q, K, V, scaled dot-product)\n",
        "□ Both models use Keras or PyTorch\n",
        "□ Both models trained with loss tracking (initial_loss and final_loss)\n",
        "□ All 4 metrics calculated for both models (MAE, RMSE, MAPE, R²)\n",
        "□ Temporal train/test split used (NO shuffling)\n",
        "□ Primary metric selected and justified\n",
        "□ Analysis written (quality matters, not just word count)\n",
        "□ Visualizations created\n",
        "□ Assignment results JSON printed at the end\n",
        "□ No execution errors in any cell\n",
        "□ File opens without corruption\n",
        "□ Submit ONLY .ipynb file (NO zip, NO data files, NO images)\n",
        "□ Screenshot of environment with account details included\n",
        "□ Only one submission attempt\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf6d14b",
      "metadata": {
        "id": "bcf6d14b"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "ENVIRONMENT VERIFICATION - SCREENSHOT REQUIRED\n",
        "================================================================================\n",
        "\n",
        "IMPORTANT: Take a screenshot of your environment showing account details\n",
        "\n",
        "For Google Colab:\n",
        "- Click on your profile icon (top right)\n",
        "- Screenshot should show your email/account clearly\n",
        "- Include the entire Colab interface with notebook name visible\n",
        "\n",
        "For BITS Virtual Lab:\n",
        "- Screenshot showing your login credentials/account details\n",
        "- Include the entire interface with your username/session info visible\n",
        "\n",
        "Paste the screenshot below this cell or in a new markdown cell.\n",
        "This helps verify the work was done by you in your environment.\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e01e42",
      "metadata": {
        "id": "16e01e42"
      },
      "outputs": [],
      "source": [
        "# Display system information\n",
        "import platform\n",
        "import sys\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a2fac2",
      "metadata": {
        "id": "c2a2fac2"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ENVIRONMENT INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n⚠️  REQUIRED: Add screenshot of your Google Colab/BITS Virtual Lab\")\n",
        "print(\"showing your account details in the cell below this one.\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}