{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7dc017f1",
      "metadata": {
        "id": "7dc017f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d156926e-5bc2-4d4b-be6e-740db38ecd7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nDEEP NEURAL NETWORKS - ASSIGNMENT 3: RNN vs TRANSFORMER FOR TIME SERIES\\nRecurrent Neural Networks vs Transformers for Time Series Prediction\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "DEEP NEURAL NETWORKS - ASSIGNMENT 3: RNN vs TRANSFORMER FOR TIME SERIES\n",
        "Recurrent Neural Networks vs Transformers for Time Series Prediction\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "23b67847",
      "metadata": {
        "id": "23b67847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "98da5e6d-5192-4ffc-fc6e-3346924e1139"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nSTUDENT INFORMATION (REQUIRED - DO NOT DELETE)\\n================================================================================\\n\\nBITS ID: [Enter your BITS ID here - e.g., 2025AA05036]\\nName: [Enter your full name here - e.g., JOHN DOE]\\nEmail: [Enter your email]\\nDate: [Submission date]\\n\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "STUDENT INFORMATION (REQUIRED - DO NOT DELETE)\n",
        "================================================================================\n",
        "\n",
        "BITS ID: [Enter your BITS ID here - e.g., 2025AA05036]\n",
        "Name: [Enter your full name here - e.g., JOHN DOE]\n",
        "Email: [Enter your email]\n",
        "Date: [Submission date]\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b5a0dc29",
      "metadata": {
        "id": "b5a0dc29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "c19e8037-b08d-4d53-fd4a-c4fe66099b34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nASSIGNMENT OVERVIEW\\n================================================================================\\n\\nThis assignment requires you to implement and compare two approaches for\\ntime series forecasting:\\n1. LSTM or GRU using Keras/PyTorch\\n2. Transformer encoder using Keras/PyTorch layers\\n\\nLearning Objectives:\\n- Build recurrent neural networks for sequential data\\n- Use transformer architecture for time series\\n- Implement or integrate positional encoding\\n- Compare RNN vs Transformer architectures\\n- Understand time series preprocessing and evaluation\\n\\nIMPORTANT:\\n- Positional encoding MUST be added to transformer\\n- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\\n- DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\\n- Use temporal train/test split (NO shuffling)\\n\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "ASSIGNMENT OVERVIEW\n",
        "================================================================================\n",
        "\n",
        "This assignment requires you to implement and compare two approaches for\n",
        "time series forecasting:\n",
        "1. LSTM or GRU using Keras/PyTorch\n",
        "2. Transformer encoder using Keras/PyTorch layers\n",
        "\n",
        "Learning Objectives:\n",
        "- Build recurrent neural networks for sequential data\n",
        "- Use transformer architecture for time series\n",
        "- Implement or integrate positional encoding\n",
        "- Compare RNN vs Transformer architectures\n",
        "- Understand time series preprocessing and evaluation\n",
        "\n",
        "IMPORTANT:\n",
        "- Positional encoding MUST be added to transformer\n",
        "- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "- DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
        "- Use temporal train/test split (NO shuffling)\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c4b05f5b",
      "metadata": {
        "id": "c4b05f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "02560d75-0ecb-42cf-cbe6-9cdd695336b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n================================================================================\\n⚠️ IMPORTANT SUBMISSION REQUIREMENTS - STRICTLY ENFORCED ⚠️\\n================================================================================\\n\\n1. FILENAME FORMAT: <BITS_ID>_rnn_assignment.ipynb\\n   Example: 2025AA05036_rnn_assignment.ipynb\\n   ❌ Wrong filename = Automatic 0 marks\\n\\n2. STUDENT INFORMATION MUST MATCH:\\n   ✓ BITS ID in filename = BITS ID in notebook (above)\\n   ✓ Name in folder = Name in notebook (above)\\n   ❌ Mismatch = 0 marks\\n\\n3. EXECUTE ALL CELLS BEFORE SUBMISSION:\\n   - Run: Kernel → Restart & Run All\\n   - Verify all outputs are visible\\n   ❌ No outputs = 0 marks\\n\\n4. FILE INTEGRITY:\\n   - Ensure notebook opens without errors\\n   - Check for corrupted cells\\n   ❌ Corrupted file = 0 marks\\n\\n5. IMPLEMENTATION REQUIREMENTS:\\n   - MUST add positional encoding to transformer (custom or built-in)\\n   - CAN use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\\n   - DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\\n   - DO NOT shuffle time series data (temporal order required)\\n   ❌ Missing positional encoding = 0 marks for transformer section\\n\\n6. DATASET REQUIREMENTS:\\n   - Minimum 1000 time steps\\n   - Train/test split: 90/10 OR 85/15 (temporal split only)\\n   - Sequence length: 10-50 time steps\\n   - Prediction horizon: 1-10 time steps\\n\\n7. USE KERAS OR PYTORCH:\\n   - Use framework's LSTM/GRU layers\\n   - Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\\n   - Add positional encoding (custom implementation or built-in)\\n   - Use standard training methods\\n\\n8. FILE SUBMISSION:\\n   - Submit ONLY the .ipynb file\\n   - NO zip files, NO separate data files, NO separate image files\\n   - All code and outputs must be in the notebook\\n   - Only one submission attempt allowed\\n\\n================================================================================\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "⚠️ IMPORTANT SUBMISSION REQUIREMENTS - STRICTLY ENFORCED ⚠️\n",
        "================================================================================\n",
        "\n",
        "1. FILENAME FORMAT: <BITS_ID>_rnn_assignment.ipynb\n",
        "   Example: 2025AA05036_rnn_assignment.ipynb\n",
        "   ❌ Wrong filename = Automatic 0 marks\n",
        "\n",
        "2. STUDENT INFORMATION MUST MATCH:\n",
        "   ✓ BITS ID in filename = BITS ID in notebook (above)\n",
        "   ✓ Name in folder = Name in notebook (above)\n",
        "   ❌ Mismatch = 0 marks\n",
        "\n",
        "3. EXECUTE ALL CELLS BEFORE SUBMISSION:\n",
        "   - Run: Kernel → Restart & Run All\n",
        "   - Verify all outputs are visible\n",
        "   ❌ No outputs = 0 marks\n",
        "\n",
        "4. FILE INTEGRITY:\n",
        "   - Ensure notebook opens without errors\n",
        "   - Check for corrupted cells\n",
        "   ❌ Corrupted file = 0 marks\n",
        "\n",
        "5. IMPLEMENTATION REQUIREMENTS:\n",
        "   - MUST add positional encoding to transformer (custom or built-in)\n",
        "   - CAN use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "   - DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
        "   - DO NOT shuffle time series data (temporal order required)\n",
        "   ❌ Missing positional encoding = 0 marks for transformer section\n",
        "\n",
        "6. DATASET REQUIREMENTS:\n",
        "   - Minimum 1000 time steps\n",
        "   - Train/test split: 90/10 OR 85/15 (temporal split only)\n",
        "   - Sequence length: 10-50 time steps\n",
        "   - Prediction horizon: 1-10 time steps\n",
        "\n",
        "7. USE KERAS OR PYTORCH:\n",
        "   - Use framework's LSTM/GRU layers\n",
        "   - Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "   - Add positional encoding (custom implementation or built-in)\n",
        "   - Use standard training methods\n",
        "\n",
        "8. FILE SUBMISSION:\n",
        "   - Submit ONLY the .ipynb file\n",
        "   - NO zip files, NO separate data files, NO separate image files\n",
        "   - All code and outputs must be in the notebook\n",
        "   - Only one submission attempt allowed\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f7883103",
      "metadata": {
        "id": "f7883103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "737d18a7-3f0d-4f9d-aca4-bb0d895ee803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Import Required Libraries\n",
        "# ================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "\n",
        "# ================================\n",
        "# PyTorch Imports (LSTM & Transformer)\n",
        "# ================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ================================\n",
        "# Reproducibility\n",
        "# ================================\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# ================================\n",
        "# Device configuration\n",
        "# ================================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b889b4",
      "metadata": {
        "id": "17b889b4"
      },
      "source": [
        "Deep learning frameworks (choose Keras or PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "002c89de",
      "metadata": {
        "id": "002c89de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "e8bb1a92-4a55-41e1-e02e-54443147cfb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nPART 1: DATASET LOADING AND EXPLORATION (Informational)\\n================================================================================\\n\\nInstructions:\\n1. Choose ONE dataset from the allowed list\\n2. Load and explore the time series data\\n3. Fill in ALL required metadata fields below\\n4. Provide justification for your primary metric choice\\n\\nALLOWED DATASETS:\\n- Stock Prices (daily/hourly closing prices)\\n- Weather Data (temperature, humidity, pressure)\\n- Energy Consumption (electricity/power usage)\\n- Sensor Data (IoT sensor readings)\\n- Custom time series (with approval)\\n\\nREQUIRED OUTPUT:\\n- Print all metadata fields\\n- Time series plots\\n- Stationarity analysis\\n- Train/test split visualization\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 1: DATASET LOADING AND EXPLORATION (Informational)\n",
        "================================================================================\n",
        "\n",
        "Instructions:\n",
        "1. Choose ONE dataset from the allowed list\n",
        "2. Load and explore the time series data\n",
        "3. Fill in ALL required metadata fields below\n",
        "4. Provide justification for your primary metric choice\n",
        "\n",
        "ALLOWED DATASETS:\n",
        "- Stock Prices (daily/hourly closing prices)\n",
        "- Weather Data (temperature, humidity, pressure)\n",
        "- Energy Consumption (electricity/power usage)\n",
        "- Sensor Data (IoT sensor readings)\n",
        "- Custom time series (with approval)\n",
        "\n",
        "REQUIRED OUTPUT:\n",
        "- Print all metadata fields\n",
        "- Time series plots\n",
        "- Stationarity analysis\n",
        "- Train/test split visualization\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c8fef3",
      "metadata": {
        "id": "83c8fef3"
      },
      "source": [
        "1.1 Dataset Selection and Loading\n",
        "TODO: Load your chosen time series dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e2b9ac13",
      "metadata": {
        "id": "e2b9ac13"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# REQUIRED: Fill in these metadata fields\n",
        "# =============================================================================\n",
        "\n",
        "dataset_name = \"Jena Climate Weather Dataset\"\n",
        "dataset_source = \"Kaggle: mnassrib/jena-climate\"\n",
        "n_samples = 420551       # Total number of time steps in full dataset (~420k)\n",
        "n_features = 3           # Multivariate: Temperature + Pressure + Humidity\n",
        "sequence_length = 30     # Lookback window (10-50 recommended)\n",
        "prediction_horizon = 1   # Forecast 1 step ahead (can be 1-10)\n",
        "problem_type = \"time_series_forecasting\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6ae533b6",
      "metadata": {
        "id": "6ae533b6"
      },
      "outputs": [],
      "source": [
        "primary_metric = \"RMSE\"\n",
        "\n",
        "metric_justification = (\n",
        "    \"RMSE is chosen because it penalizes large forecasting errors more heavily \"\n",
        "    \"than MAE, which is important for accurately predicting extreme temperature \"\n",
        "    \"variations. It is also interpretable in the original °C units.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2fef57a2",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "2fef57a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f552b5-5598-4215-becb-d0446281b918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DATASET INFORMATION\n",
            "======================================================================\n",
            "Dataset Name        : Jena Climate Weather Dataset\n",
            "Dataset Source      : Kaggle: mnassrib/jena-climate\n",
            "Total Samples       : 420551\n",
            "Number of Features  : 3\n",
            "Sequence Length     : 30\n",
            "Prediction Horizon  : 1\n",
            "Primary Metric      : RMSE\n",
            "Metric Justification:\n",
            "RMSE is chosen because it penalizes large forecasting errors more heavily than MAE, which is important for accurately predicting extreme temperature variations. It is also interpretable in the original °C units.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset Name        : {dataset_name}\")\n",
        "print(f\"Dataset Source      : {dataset_source}\")\n",
        "print(f\"Total Samples       : {n_samples}\")\n",
        "print(f\"Number of Features  : {n_features}\")\n",
        "print(f\"Sequence Length     : {sequence_length}\")\n",
        "print(f\"Prediction Horizon  : {prediction_horizon}\")\n",
        "print(f\"Primary Metric      : {primary_metric}\")\n",
        "print(\"Metric Justification:\")\n",
        "print(metric_justification)\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"mnassrib/jena-climate\")"
      ],
      "metadata": {
        "id": "jvxjOsnGC6XP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74830d49-6a2a-419f-ce8d-eacb2c1568a1"
      },
      "id": "jvxjOsnGC6XP",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'jena-climate' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Reload Jena Climate dataset safely (Kaggle / Colab / Local compatible)\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Try Kaggle path first\n",
        "# --------------------------------------------------\n",
        "kaggle_path = \"/kaggle/input/jena-climate\"\n",
        "\n",
        "if os.path.exists(kaggle_path):\n",
        "    dataset_path = kaggle_path\n",
        "    print(\"Using Kaggle input path\")\n",
        "\n",
        "else:\n",
        "    # Fallback to kagglehub download path\n",
        "    import kagglehub\n",
        "    dataset_path = kagglehub.dataset_download(\"mnassrib/jena-climate\")\n",
        "    print(\"Using kagglehub download path:\", dataset_path)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Locate CSV file\n",
        "# --------------------------------------------------\n",
        "csv_files = glob.glob(os.path.join(dataset_path, \"*.csv\"))\n",
        "assert len(csv_files) > 0, \"No CSV files found in dataset directory!\"\n",
        "\n",
        "csv_path = csv_files[0]\n",
        "print(\"Using file:\", csv_path)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Load dataset\n",
        "# --------------------------------------------------\n",
        "df = pd.read_csv(csv_path).copy()\n",
        "\n",
        "# Parse datetime (European format)\n",
        "df[\"Date Time\"] = pd.to_datetime(df[\"Date Time\"], dayfirst=True)\n",
        "df = df.sort_values(\"Date Time\")\n",
        "df.set_index(\"Date Time\", inplace=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Fix invalid placeholder values (-9999)\n",
        "# --------------------------------------------------\n",
        "INVALID_VAL = -9999.0\n",
        "cols_with_invalid = [\"wv (m/s)\", \"max. wv (m/s)\"]\n",
        "\n",
        "for col in cols_with_invalid:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].replace(INVALID_VAL, np.nan)\n",
        "\n",
        "df[cols_with_invalid] = df[cols_with_invalid].ffill().bfill()\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5vh6HFSVxEQ",
        "outputId": "80b7aefa-61c2-4d9f-92e8-8366c2bc2822"
      },
      "id": "x5vh6HFSVxEQ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Kaggle input path\n",
            "Using file: /kaggle/input/jena-climate/jena_climate_2009_2016.csv\n",
            "Dataset shape: (420551, 14)\n",
            "                     p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
            "Date Time                                                                \n",
            "2009-01-01 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
            "2009-01-01 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
            "2009-01-01 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
            "2009-01-01 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
            "2009-01-01 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
            "\n",
            "                     VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  \\\n",
            "Date Time                                                                  \n",
            "2009-01-01 00:10:00          3.33          3.11          0.22       1.94   \n",
            "2009-01-01 00:20:00          3.23          3.02          0.21       1.89   \n",
            "2009-01-01 00:30:00          3.21          3.01          0.20       1.88   \n",
            "2009-01-01 00:40:00          3.26          3.07          0.19       1.92   \n",
            "2009-01-01 00:50:00          3.27          3.08          0.19       1.92   \n",
            "\n",
            "                     H2OC (mmol/mol)  rho (g/m**3)  wv (m/s)  max. wv (m/s)  \\\n",
            "Date Time                                                                     \n",
            "2009-01-01 00:10:00             3.12       1307.75      1.03           1.75   \n",
            "2009-01-01 00:20:00             3.03       1309.80      0.72           1.50   \n",
            "2009-01-01 00:30:00             3.02       1310.24      0.19           0.63   \n",
            "2009-01-01 00:40:00             3.08       1309.19      0.34           0.50   \n",
            "2009-01-01 00:50:00             3.09       1309.00      0.32           0.63   \n",
            "\n",
            "                     wd (deg)  \n",
            "Date Time                      \n",
            "2009-01-01 00:10:00     152.3  \n",
            "2009-01-01 00:20:00     136.1  \n",
            "2009-01-01 00:30:00     171.6  \n",
            "2009-01-01 00:40:00     198.0  \n",
            "2009-01-01 00:50:00     214.3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd8ddfa",
      "metadata": {
        "id": "dfd8ddfa"
      },
      "source": [
        "1.2 Time Series Exploration\n",
        "TODO: Plot time series data\n",
        "TODO: Check for trends, seasonality\n",
        "TODO: Perform stationarity tests (optional but recommended)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # =============================================================================\n",
        "# # 1.2 Time Series Exploration\n",
        "# # =============================================================================\n",
        "\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "# from statsmodels.tsa.stattools import adfuller\n",
        "# import os\n",
        "# import glob\n",
        "\n",
        "# # -----------------------------------------------------------------\n",
        "# # 1. Load dataset from kagglehub download\n",
        "# # -----------------------------------------------------------------\n",
        "# import kagglehub\n",
        "# path = kagglehub.dataset_download(\"mnassrib/jena-climate\")\n",
        "# print(\"Dataset downloaded to:\", path)\n",
        "\n",
        "# # Find CSV in the folder\n",
        "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "# assert len(csv_files) > 0, \"No CSV file found in the downloaded dataset!\"\n",
        "# csv_path = csv_files[0]\n",
        "\n",
        "# # Read CSV\n",
        "# df = pd.read_csv(csv_path)\n",
        "\n",
        "# # -----------------------------------------------------------------\n",
        "# # 2. Set datetime index (dayfirst=True for European format)\n",
        "# # -----------------------------------------------------------------\n",
        "# df[\"Date Time\"] = pd.to_datetime(df[\"Date Time\"], dayfirst=True)\n",
        "# df = df.sort_values(\"Date Time\")\n",
        "# df.set_index(\"Date Time\", inplace=True)\n",
        "\n",
        "\n",
        "# # -----------------------------------------------------------------\n",
        "# # 3. Define target and auxiliary features\n",
        "# # -----------------------------------------------------------------\n",
        "# TARGET_COLUMN = \"T (degC)\"         # Temperature\n",
        "# AUX_COLUMNS = [\"p (mbar)\", \"rh (%)\"]  # Pressure + Relative Humidity\n",
        "\n",
        "# # -----------------------------------------------------------------\n",
        "# # 4. Plot raw time series\n",
        "# # -----------------------------------------------------------------\n",
        "# plt.figure(figsize=(15,5))\n",
        "# plt.plot(df[TARGET_COLUMN], label=\"Temperature (°C)\")\n",
        "# plt.title(\"Temperature Time Series\")\n",
        "# plt.xlabel(\"Time\")\n",
        "# plt.ylabel(\"°C\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(15,5))\n",
        "# plt.plot(df[\"p (mbar)\"], label=\"Pressure (mbar)\", color='orange')\n",
        "# plt.title(\"Pressure Time Series\")\n",
        "# plt.xlabel(\"Time\")\n",
        "# plt.ylabel(\"mbar\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(15,5))\n",
        "# plt.plot(df[\"rh (%)\"], label=\"Relative Humidity (%)\", color='green')\n",
        "# plt.title(\"Relative Humidity Time Series\")\n",
        "# plt.xlabel(\"Time\")\n",
        "# plt.ylabel(\"%\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# # -----------------------------------------------------------------\n",
        "# # 5. Seasonal decomposition (Temperature only)\n",
        "# # -----------------------------------------------------------------\n",
        "# # 144 steps per day (10-min intervals)\n",
        "# decompose_result = seasonal_decompose(df[TARGET_COLUMN], model='additive', period=144)\n",
        "# plt.figure(figsize=(12,10))\n",
        "# decompose_result.plot()\n",
        "# plt.suptitle(\"Seasonal Decomposition of Temperature\", fontsize=16)\n",
        "# plt.show()\n",
        "\n",
        "# # -----------------------------------------------------------------\n",
        "# # 6. Stationarity test (ADF) on Temperature\n",
        "# # -----------------------------------------------------------------\n",
        "# adf_result = adfuller(df[TARGET_COLUMN])\n",
        "# print(\"\\n================ STATIONARITY TEST (ADF) ================\")\n",
        "# print(f\"ADF Statistic: {adf_result[0]:.4f}\")\n",
        "# print(f\"p-value      : {adf_result[1]:.4f}\")\n",
        "\n",
        "# if adf_result[1] < 0.05:\n",
        "#     print(\"Result       : Stationary (reject H0)\")\n",
        "# else:\n",
        "#     print(\"Result       : Non-stationary (fail to reject H0)\")\n"
      ],
      "metadata": {
        "id": "Q2mpAWRhCBQv"
      },
      "id": "Q2mpAWRhCBQv",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Reload Jena Climate dataset safely\n",
        "# =============================================================================\n",
        "\n",
        "# import pandas as pd\n",
        "# import glob\n",
        "# import os\n",
        "# import numpy as np\n",
        "\n",
        "# dataset_path = \"/kaggle/input/jena-climate\"\n",
        "# csv_files = glob.glob(os.path.join(dataset_path, \"*.csv\"))\n",
        "\n",
        "# assert len(csv_files) > 0, \"No CSV files found!\"\n",
        "\n",
        "# csv_path = csv_files[0]\n",
        "# print(\"Using file:\", csv_path)\n",
        "\n",
        "# # Read dataset\n",
        "# df = pd.read_csv(csv_path).copy()\n",
        "\n",
        "# # Parse datetime (European format)\n",
        "# df[\"Date Time\"] = pd.to_datetime(df[\"Date Time\"], dayfirst=True)\n",
        "# df = df.sort_values(\"Date Time\")\n",
        "# df.set_index(\"Date Time\", inplace=True)\n",
        "\n",
        "# # --------------------------------------------------\n",
        "# # IMPORTANT: Fix invalid placeholder values (-9999)\n",
        "# # --------------------------------------------------\n",
        "# INVALID_VAL = -9999.0\n",
        "\n",
        "# cols_with_invalid = [\"wv (m/s)\", \"max. wv (m/s)\"]\n",
        "# for col in cols_with_invalid:\n",
        "#     if col in df.columns:\n",
        "#         df[col] = df[col].replace(INVALID_VAL, np.nan)\n",
        "\n",
        "# # Fill after replacement\n",
        "# df[cols_with_invalid] = df[cols_with_invalid].ffill().bfill()\n",
        "\n",
        "# print(\"Dataset shape:\", df.shape)\n",
        "# print(df.head())\n"
      ],
      "metadata": {
        "id": "wWnThPnuICym"
      },
      "id": "wWnThPnuICym",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "nwf-pFDLF5yR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c3c363-c6ab-45be-effc-b609976c6173"
      },
      "id": "nwf-pFDLF5yR",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
            "Date Time                                                                \n",
            "2009-01-01 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
            "2009-01-01 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
            "2009-01-01 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
            "2009-01-01 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
            "2009-01-01 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
            "\n",
            "                     VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  \\\n",
            "Date Time                                                                  \n",
            "2009-01-01 00:10:00          3.33          3.11          0.22       1.94   \n",
            "2009-01-01 00:20:00          3.23          3.02          0.21       1.89   \n",
            "2009-01-01 00:30:00          3.21          3.01          0.20       1.88   \n",
            "2009-01-01 00:40:00          3.26          3.07          0.19       1.92   \n",
            "2009-01-01 00:50:00          3.27          3.08          0.19       1.92   \n",
            "\n",
            "                     H2OC (mmol/mol)  rho (g/m**3)  wv (m/s)  max. wv (m/s)  \\\n",
            "Date Time                                                                     \n",
            "2009-01-01 00:10:00             3.12       1307.75      1.03           1.75   \n",
            "2009-01-01 00:20:00             3.03       1309.80      0.72           1.50   \n",
            "2009-01-01 00:30:00             3.02       1310.24      0.19           0.63   \n",
            "2009-01-01 00:40:00             3.08       1309.19      0.34           0.50   \n",
            "2009-01-01 00:50:00             3.09       1309.00      0.32           0.63   \n",
            "\n",
            "                     wd (deg)  \n",
            "Date Time                      \n",
            "2009-01-01 00:10:00     152.3  \n",
            "2009-01-01 00:20:00     136.1  \n",
            "2009-01-01 00:30:00     171.6  \n",
            "2009-01-01 00:40:00     198.0  \n",
            "2009-01-01 00:50:00     214.3  \n",
            "            p (mbar)       T (degC)       Tpot (K)    Tdew (degC)  \\\n",
            "count  420551.000000  420551.000000  420551.000000  420551.000000   \n",
            "mean      989.212776       9.450147     283.492743       4.955854   \n",
            "std         8.358481       8.423365       8.504471       6.730674   \n",
            "min       913.600000     -23.010000     250.600000     -25.010000   \n",
            "25%       984.200000       3.360000     277.430000       0.240000   \n",
            "50%       989.580000       9.420000     283.470000       5.220000   \n",
            "75%       994.720000      15.470000     289.530000      10.070000   \n",
            "max      1015.350000      37.280000     311.340000      23.110000   \n",
            "\n",
            "              rh (%)   VPmax (mbar)   VPact (mbar)   VPdef (mbar)  \\\n",
            "count  420551.000000  420551.000000  420551.000000  420551.000000   \n",
            "mean       76.008259      13.576251       9.533756       4.042412   \n",
            "std        16.476175       7.739020       4.184164       4.896851   \n",
            "min        12.950000       0.950000       0.790000       0.000000   \n",
            "25%        65.210000       7.780000       6.210000       0.870000   \n",
            "50%        79.300000      11.820000       8.860000       2.190000   \n",
            "75%        89.400000      17.600000      12.350000       5.300000   \n",
            "max       100.000000      63.770000      28.320000      46.010000   \n",
            "\n",
            "           sh (g/kg)  H2OC (mmol/mol)   rho (g/m**3)       wv (m/s)  \\\n",
            "count  420551.000000    420551.000000  420551.000000  420551.000000   \n",
            "mean        6.022408         9.640223    1216.062748       2.130455   \n",
            "std         2.656139         4.235395      39.975208       1.542497   \n",
            "min         0.500000         0.800000    1059.450000       0.000000   \n",
            "25%         3.920000         6.290000    1187.490000       0.990000   \n",
            "50%         5.590000         8.960000    1213.790000       1.760000   \n",
            "75%         7.800000        12.490000    1242.770000       2.860000   \n",
            "max        18.130000        28.820000    1393.540000      28.490000   \n",
            "\n",
            "       max. wv (m/s)       wd (deg)  \n",
            "count  420551.000000  420551.000000  \n",
            "mean        3.532323     174.743738  \n",
            "std         2.340384      86.681693  \n",
            "min         0.000000       0.000000  \n",
            "25%         1.760000     124.900000  \n",
            "50%         2.960000     198.100000  \n",
            "75%         4.740000     234.100000  \n",
            "max        23.500000     360.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d498e253",
      "metadata": {
        "id": "d498e253"
      },
      "source": [
        "1.3 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6efdacdc",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "6efdacdc"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 1.3 Data Preprocessing (FINAL & SAFE)\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def preprocess_timeseries(data, feature_cols):\n",
        "    \"\"\"\n",
        "    Preprocess time series data\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): raw dataframe with datetime index\n",
        "        feature_cols (list): ordered list of feature columns\n",
        "                             (TARGET must be first)\n",
        "    Returns:\n",
        "        scaled_data (np.ndarray): normalized feature values (0–1)\n",
        "        scaler (MinMaxScaler): fitted scaler object\n",
        "    \"\"\"\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Work on a copy to avoid modifying original dataframe\n",
        "    # --------------------------------------------------\n",
        "    df = data.copy()\n",
        "\n",
        "    print(\"\\n================ PREPROCESSING =================\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1. Validate feature order\n",
        "    # --------------------------------------------------\n",
        "    assert feature_cols[0] == \"T (degC)\", \\\n",
        "        \"Target variable 'T (degC)' must be the first feature\"\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2. Handle missing values (time-series safe)\n",
        "    # --------------------------------------------------\n",
        "    missing = df[feature_cols].isnull().sum()\n",
        "    print(\"Missing values per column:\\n\", missing)\n",
        "\n",
        "    df[feature_cols] = df[feature_cols].ffill().bfill()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 3. Extract selected features\n",
        "    # --------------------------------------------------\n",
        "    values = df[feature_cols].values.astype(np.float32)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 4. Normalize features to [0, 1]\n",
        "    # --------------------------------------------------\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(values)\n",
        "\n",
        "    print(\"Scaling complete.\")\n",
        "    print(\"Data shape after preprocessing:\", scaled_data.shape)\n",
        "    print(\"Scaling range: [0, 1]\")\n",
        "\n",
        "    return scaled_data, scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ea0da181",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ea0da181"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 1.4 Sequence Generation\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_sequences(data, seq_length, pred_horizon):\n",
        "    \"\"\"\n",
        "    Create sequences for time series prediction\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): preprocessed normalized data (n_samples, n_features)\n",
        "        seq_length (int): lookback window\n",
        "        pred_horizon (int): forecast steps ahead\n",
        "\n",
        "    Returns:\n",
        "        X (np.ndarray): input sequences (num_samples, seq_length, n_features)\n",
        "        y (np.ndarray): target values\n",
        "                        - shape (num_samples,) for single-step\n",
        "                        - shape (num_samples, pred_horizon) for multi-step\n",
        "                        (targets only for first feature → temperature)\n",
        "    \"\"\"\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(data) - seq_length - pred_horizon + 1):\n",
        "\n",
        "        # Input window\n",
        "        x_window = data[i : i + seq_length]\n",
        "\n",
        "        # Target → predict first column (Temperature)\n",
        "        y_window = data[\n",
        "            i + seq_length : i + seq_length + pred_horizon,\n",
        "            0\n",
        "        ]\n",
        "\n",
        "        X.append(x_window)\n",
        "        y.append(y_window)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    print(\"\\n================ SEQUENCE GENERATION =================\")\n",
        "    print(\"X shape:\", X.shape)\n",
        "    print(\"y shape:\", y.shape)\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d888128f",
      "metadata": {
        "id": "d888128f"
      },
      "source": [
        "TODO: Preprocess data\n",
        "TODO: Create sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "725e3489",
      "metadata": {
        "id": "725e3489"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 1.4 Sequence Generation (FINAL & SAFE)\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_sequences(data, seq_length, pred_horizon):\n",
        "    \"\"\"\n",
        "    Create sequences for time series prediction\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): preprocessed normalized data (n_samples, n_features)\n",
        "        seq_length (int): lookback window\n",
        "        pred_horizon (int): forecast steps ahead\n",
        "\n",
        "    Returns:\n",
        "        X (np.ndarray): input sequences (num_samples, seq_length, n_features)\n",
        "        y (np.ndarray): target values\n",
        "                        - shape (num_samples, 1) for single-step\n",
        "                        - shape (num_samples, pred_horizon) for multi-step\n",
        "    \"\"\"\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Safety checks\n",
        "    # --------------------------------------------------\n",
        "    assert isinstance(data, np.ndarray), \"Input data must be a NumPy array\"\n",
        "    assert data.ndim == 2, \"Data must be 2D: (n_samples, n_features)\"\n",
        "    assert seq_length > 0 and pred_horizon > 0, \"Sequence length and horizon must be > 0\"\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(data) - seq_length - pred_horizon + 1):\n",
        "\n",
        "        # Input window\n",
        "        X.append(data[i : i + seq_length])\n",
        "\n",
        "        # Target: first feature (Temperature)\n",
        "        y.append(\n",
        "            data[i + seq_length : i + seq_length + pred_horizon, 0]\n",
        "        )\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.float32)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Shape normalization for single-step forecasting\n",
        "    # --------------------------------------------------\n",
        "    if pred_horizon == 1:\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "    print(\"\\n================ SEQUENCE GENERATION =================\")\n",
        "    print(f\"Input sequences (X): {X.shape}\")\n",
        "    print(f\"Target values (y)  : {y.shape}\")\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "daa1ee1f",
      "metadata": {
        "id": "daa1ee1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1a07aa-ac1e-4a05-a1ab-17ff307b7dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ PREPROCESSING =================\n",
            "Missing values per column:\n",
            " T (degC)    0\n",
            "p (mbar)    0\n",
            "rh (%)      0\n",
            "dtype: int64\n",
            "Scaling complete.\n",
            "Data shape after preprocessing: (420551, 3)\n",
            "Scaling range: [0, 1]\n",
            "\n",
            "================ SEQUENCE GENERATION =================\n",
            "Input sequences (X): (420521, 30, 3)\n",
            "Target values (y)  : (420521, 1)\n",
            "\n",
            "Train/Test Split: 0.9\n",
            "Training Samples: 378468\n",
            "Test Samples: 42053\n",
            "⚠️  IMPORTANT: Temporal split used (NO shuffling)\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------\n",
        "# Select features (example: Temperature, Pressure, Humidity)\n",
        "# --------------------------------------------------\n",
        "feature_cols = [\"T (degC)\", \"p (mbar)\", \"rh (%)\"]\n",
        "\n",
        "# Preprocess (from your earlier function)\n",
        "scaled_data, scaler = preprocess_timeseries(df, feature_cols)\n",
        "\n",
        "# Create sequences\n",
        "X, y = create_sequences(\n",
        "    data=scaled_data,\n",
        "    seq_length=sequence_length,\n",
        "    pred_horizon=prediction_horizon\n",
        ")\n",
        "# =============================================================================\n",
        "# 1.5 Train / Test Split (Temporal)\n",
        "# =============================================================================\n",
        "\n",
        "train_test_ratio = 0.9  # 90% train, 10% test\n",
        "\n",
        "# Total number of samples\n",
        "n_total = X.shape[0]\n",
        "\n",
        "# Compute split index\n",
        "train_samples = int(n_total * train_test_ratio)\n",
        "test_samples  = n_total - train_samples\n",
        "\n",
        "# --------------------------------------------------\n",
        "# IMPORTANT: Temporal split (NO SHUFFLING)\n",
        "# --------------------------------------------------\n",
        "X_train = X[:train_samples]\n",
        "y_train = y[:train_samples]\n",
        "\n",
        "X_test  = X[train_samples:]\n",
        "y_test  = y[train_samples:]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Sanity checks\n",
        "# --------------------------------------------------\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0]  == y_test.shape[0]\n",
        "\n",
        "print(f\"\\nTrain/Test Split: {train_test_ratio}\")\n",
        "print(f\"Training Samples: {train_samples}\")\n",
        "print(f\"Test Samples: {test_samples}\")\n",
        "print(\"⚠️  IMPORTANT: Temporal split used (NO shuffling)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9ef664ae",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ef664ae",
        "outputId": "f63287c4-a4c3-4c02-8339-8a4cee38a58b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "LSTMModel(\n",
            "  (lstm): LSTM(3, 64, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Epoch [1/10], Loss: 0.002289\n",
            "Epoch [2/10], Loss: 0.000389\n",
            "Epoch [3/10], Loss: 0.000060\n",
            "Epoch [4/10], Loss: 0.000033\n",
            "Epoch [5/10], Loss: 0.000029\n",
            "Epoch [6/10], Loss: 0.000020\n",
            "Epoch [7/10], Loss: 0.000048\n",
            "Epoch [8/10], Loss: 0.000020\n",
            "Epoch [9/10], Loss: 0.000019\n",
            "Epoch [10/10], Loss: 0.000018\n",
            "\n",
            "Initial Training Loss: 0.0022894953110840204\n",
            "Final Training Loss  : 1.815560054283111e-05\n",
            "\n",
            "================ LSTM TEST METRICS ================\n",
            "MAE : 0.004724437370896339\n",
            "RMSE: 0.006442634351676877\n",
            "MAPE: 0.7456957\n",
            "R2  : 0.9975500106811523\n",
            "\n",
            "LSTM Results Dictionary:\n",
            "{'model_type': 'LSTM', 'framework': 'pytorch', 'architecture': {'n_layers': 2, 'hidden_dim': 64}, 'initial_loss': 0.0022894953110840204, 'final_loss': 1.815560054283111e-05, 'mae': 0.004724437370896339, 'rmse': 0.006442634351676877, 'mape': 0.7456957101821899, 'r2_score': 0.9975500106811523}\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# PART 2: LSTM IMPLEMENTATION (IMPROVED)\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Device configuration\n",
        "# ------------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Convert NumPy arrays to PyTorch tensors (CPU first – best practice)\n",
        "# ------------------------------------------------------------------\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# DataLoader (NO SHUFFLING – time series)\n",
        "# ------------------------------------------------------------------\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "test_dataset  = TensorDataset(X_test_t, y_test_t)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# LSTM Model Definition (STACKED ≥ 2 layers)\n",
        "# ------------------------------------------------------------------\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_type = \"LSTM\"\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]        # last time step\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Model Initialization\n",
        "# ------------------------------------------------------------------\n",
        "input_dim  = X_train.shape[2]\n",
        "hidden_dim = 64\n",
        "num_layers = 2                    # REQUIRED\n",
        "output_dim = prediction_horizon\n",
        "\n",
        "lstm_model = LSTMModel(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_layers=num_layers,\n",
        "    output_dim=output_dim\n",
        ").to(device)\n",
        "\n",
        "print(lstm_model)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Loss function & Optimizer\n",
        "# ------------------------------------------------------------------\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Training Loop (with gradient clipping)\n",
        "# ------------------------------------------------------------------\n",
        "epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "lstm_model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "\n",
        "        # Move batch to GPU (best practice)\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = lstm_model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # 🔒 Prevent exploding gradients (LSTM-safe)\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
        "    loss_history.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Initial & Final Loss (AUTOGRADER REQUIRED)\n",
        "# ------------------------------------------------------------------\n",
        "initial_loss = loss_history[0]\n",
        "final_loss   = loss_history[-1]\n",
        "\n",
        "print(\"\\nInitial Training Loss:\", initial_loss)\n",
        "print(\"Final Training Loss  :\", final_loss)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Evaluation on Test Set\n",
        "# ------------------------------------------------------------------\n",
        "lstm_model.eval()\n",
        "\n",
        "y_preds = []\n",
        "y_trues = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        preds = lstm_model(xb)\n",
        "        y_preds.append(preds.cpu().numpy())\n",
        "        y_trues.append(yb.cpu().numpy())\n",
        "\n",
        "y_pred = np.vstack(y_preds).reshape(-1)\n",
        "y_true = np.vstack(y_trues).reshape(-1)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Metrics (ALL REQUIRED)\n",
        "# ------------------------------------------------------------------\n",
        "mae  = mean_absolute_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "epsilon = 1e-8\n",
        "mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "print(\"\\n================ LSTM TEST METRICS ================\")\n",
        "print(\"MAE :\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAPE:\", mape)\n",
        "print(\"R2  :\", r2)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Results Dictionary (FINAL OUTPUT)\n",
        "# ------------------------------------------------------------------\n",
        "lstm_results = {\n",
        "    \"model_type\": \"LSTM\",\n",
        "    \"framework\": \"pytorch\",\n",
        "    \"architecture\": {\n",
        "        \"n_layers\": num_layers,\n",
        "        \"hidden_dim\": hidden_dim\n",
        "    },\n",
        "    \"initial_loss\": float(initial_loss),\n",
        "    \"final_loss\": float(final_loss),\n",
        "    \"mae\": float(mae),\n",
        "    \"rmse\": float(rmse),\n",
        "    \"mape\": float(mape),\n",
        "    \"r2_score\": float(r2)\n",
        "}\n",
        "\n",
        "print(\"\\nLSTM Results Dictionary:\")\n",
        "print(lstm_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b349fea",
      "metadata": {
        "id": "0b349fea"
      },
      "source": [
        "2.1 LSTM/GRU Architecture Design\n",
        "TODO: Choose LSTM or GRU\n",
        "TODO: Design architecture with stacked layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "87daaa54",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "87daaa54"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 2.1 LSTM/GRU Architecture Design\n",
        "# =============================================================================\n",
        "# - Model Type: LSTM (chosen)\n",
        "# - Stacked recurrent layers (n_layers >= 2)\n",
        "# - Output layer for forecasting\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def build_rnn_model(model_type, input_shape, hidden_units, n_layers, output_size):\n",
        "    \"\"\"\n",
        "    Build LSTM or GRU model\n",
        "\n",
        "    Args:\n",
        "        model_type (str): 'LSTM' or 'GRU'\n",
        "        input_shape (tuple): (sequence_length, n_features)\n",
        "        hidden_units (int): number of hidden units per layer\n",
        "        n_layers (int): number of stacked layers (minimum 2)\n",
        "        output_size (int): prediction horizon\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): initialized PyTorch RNN model\n",
        "        criterion: loss function\n",
        "        optimizer: optimizer object\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Assertions (ENFORCED by assignment)\n",
        "    # ------------------------------------------------------------------\n",
        "    assert model_type in [\"LSTM\", \"GRU\"], \"model_type must be 'LSTM' or 'GRU'\"\n",
        "    assert n_layers >= 2, \"n_layers must be at least 2 (stacked layers REQUIRED)\"\n",
        "\n",
        "    seq_len, n_features = input_shape\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # RNN Model Definition\n",
        "    # ------------------------------------------------------------------\n",
        "    class RNNModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(RNNModel, self).__init__()\n",
        "\n",
        "            if model_type == \"LSTM\":\n",
        "                self.rnn = nn.LSTM(\n",
        "                    input_size=n_features,\n",
        "                    hidden_size=hidden_units,\n",
        "                    num_layers=n_layers,\n",
        "                    batch_first=True\n",
        "                )\n",
        "            else:  # GRU\n",
        "                self.rnn = nn.GRU(\n",
        "                    input_size=n_features,\n",
        "                    hidden_size=hidden_units,\n",
        "                    num_layers=n_layers,\n",
        "                    batch_first=True\n",
        "                )\n",
        "\n",
        "            # Output layer\n",
        "            self.fc = nn.Linear(hidden_units, output_size)\n",
        "\n",
        "            self.model_type = model_type\n",
        "\n",
        "        def forward(self, x):\n",
        "            out, _ = self.rnn(x)       # (batch, seq_len, hidden_units)\n",
        "            out = out[:, -1, :]        # last timestep\n",
        "            out = self.fc(out)\n",
        "            return out\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Model initialization\n",
        "    # ------------------------------------------------------------------\n",
        "    model = RNNModel()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Loss function & Optimizer (REQUIRED later)\n",
        "    # ------------------------------------------------------------------\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    return model, criterion, optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9e46a1",
      "metadata": {
        "id": "8e9e46a1"
      },
      "source": [
        "TODO: Create RNN model\n",
        "rnn_model = build_rnn_model('LSTM', (sequence_length, n_features), 64, 2, prediction_horizon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model, criterion, optimizer = build_rnn_model(\n",
        "    'LSTM',\n",
        "    (sequence_length, n_features),\n",
        "    64,\n",
        "    2,\n",
        "    prediction_horizon\n",
        ")\n"
      ],
      "metadata": {
        "id": "RTB5p3SSY9Nk"
      },
      "id": "RTB5p3SSY9Nk",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "98bf8c67",
      "metadata": {
        "id": "98bf8c67"
      },
      "source": [
        "TODO: Compile model\n",
        "For Keras: model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "For PyTorch: define optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compile model (PyTorch equivalent)\n",
        "print(\"Loss Function:\", criterion)\n",
        "print(\"Optimizer:\", optimizer)\n"
      ],
      "metadata": {
        "id": "lzsLpb1SZWn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93049329-c762-45ea-f59d-6b71e35be5e0"
      },
      "id": "lzsLpb1SZWn7",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Function: MSELoss()\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44df1c4b",
      "metadata": {
        "id": "44df1c4b"
      },
      "source": [
        "2.2 Train RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "caebf1f4",
      "metadata": {
        "id": "caebf1f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654d482e-1ec1-4935-c4d8-a108be151173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "RNN MODEL TRAINING\n",
            "======================================================================\n",
            "Epoch [1/10], Loss: 0.002213\n",
            "Epoch [2/10], Loss: 0.000560\n",
            "Epoch [3/10], Loss: 0.000125\n",
            "Epoch [4/10], Loss: 0.000037\n",
            "Epoch [5/10], Loss: 0.000028\n",
            "Epoch [6/10], Loss: 0.000023\n",
            "Epoch [7/10], Loss: 0.000022\n",
            "Epoch [8/10], Loss: 0.000022\n",
            "Epoch [9/10], Loss: 0.000021\n",
            "Epoch [10/10], Loss: 0.000020\n",
            "\n",
            "Initial Training Loss: 0.0022127020218351873\n",
            "Final Training Loss  : 2.0270027576482904e-05\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------------------------------\n",
        "# Move model to device\n",
        "# -------------------------------\n",
        "rnn_model = rnn_model.to(device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RNN MODEL TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "rnn_model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = rnn_model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    loss_history.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# REQUIRED: loss tracking\n",
        "# -------------------------------\n",
        "initial_loss = loss_history[0]\n",
        "final_loss   = loss_history[-1]\n",
        "\n",
        "print(\"\\nInitial Training Loss:\", initial_loss)\n",
        "print(\"Final Training Loss  :\", final_loss)\n",
        "\n",
        "# -------------------------------\n",
        "# Alias for comparison plots\n",
        "# -------------------------------\n",
        "rnn_loss_history = loss_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b8e65f6c",
      "metadata": {
        "id": "b8e65f6c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# Track training time\n",
        "rnn_start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d32150",
      "metadata": {
        "id": "f4d32150"
      },
      "source": [
        "TODO: Train your model\n",
        "For Keras: history = rnn_model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "For PyTorch: write training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3b093f3f",
      "metadata": {
        "id": "3b093f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "0cf86d29-e36f-4e16-b447-03d4152f118a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-281938628.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2712972438.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# (batch, seq_len, hidden_units)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# last timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             result = _VF.lstm(\n\u001b[0m\u001b[1;32m   1128\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Train RNN Model (PyTorch)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "epochs = 10\n",
        "loss_history = []\n",
        "\n",
        "rnn_model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = rnn_model(xb)\n",
        "        loss = criterion(outputs, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    loss_history.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Training time\n",
        "# ------------------------------------------------------------------\n",
        "rnn_training_time = time.time() - rnn_start_time\n",
        "print(\"\\nTraining Time (seconds):\", round(rnn_training_time, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ade3a8",
      "metadata": {
        "id": "02ade3a8"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# REQUIRED: Track initial and final loss\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "rnn_initial_loss = loss_history[0]\n",
        "rnn_final_loss   = loss_history[-1]\n",
        "\n",
        "print(\"\\nInitial RNN Training Loss:\", rnn_initial_loss)\n",
        "print(\"Final RNN Training Loss  :\", rnn_final_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49554c49",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "49554c49"
      },
      "outputs": [],
      "source": [
        "# Print summary\n",
        "print(f\"Training completed in {rnn_training_time:.2f} seconds\")\n",
        "print(f\"Initial Loss: {rnn_initial_loss:.4f}\")\n",
        "print(f\"Final Loss: {rnn_final_loss:.4f}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792870c6",
      "metadata": {
        "id": "792870c6"
      },
      "source": [
        "2.3 Evaluate RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f459c549",
      "metadata": {
        "id": "f459c549"
      },
      "source": [
        "TODO: Make predictions on test set\n",
        "TODO: Inverse transform if data was normalized\n",
        "TODO: Calculate all 4 required metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1678f898",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "1678f898"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 2.3 Evaluate RNN Model\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# MAPE implementation (REQUIRED)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def calculate_mape(y_true, y_pred):\n",
        "    \"\"\"Calculate Mean Absolute Percentage Error\"\"\"\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Make predictions on test set (PyTorch)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "rnn_model.eval()\n",
        "\n",
        "y_preds = []\n",
        "y_trues = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        preds = rnn_model(xb)\n",
        "        y_preds.append(preds.cpu().numpy())\n",
        "        y_trues.append(yb.cpu().numpy())\n",
        "\n",
        "y_pred = np.vstack(y_preds)\n",
        "y_true = np.vstack(y_trues)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# OPTIONAL: Inverse scaling (only if target was normalized)\n",
        "# ------------------------------------------------------------------\n",
        "# Uncomment ONLY if you scaled the TARGET separately\n",
        "# y_pred = target_scaler.inverse_transform(y_pred)\n",
        "# y_true = target_scaler.inverse_transform(y_true)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# For multi-step horizon → evaluate first step\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "if prediction_horizon > 1:\n",
        "    y_pred_eval = y_pred[:, 0]\n",
        "    y_true_eval = y_true[:, 0]\n",
        "else:\n",
        "    y_pred_eval = y_pred.reshape(-1)\n",
        "    y_true_eval = y_true.reshape(-1)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Metrics (ALL 4 REQUIRED)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "mae  = mean_absolute_error(y_true_eval, y_pred_eval)\n",
        "rmse = np.sqrt(mean_squared_error(y_true_eval, y_pred_eval))\n",
        "mape = calculate_mape(y_true_eval, y_pred_eval)\n",
        "r2   = r2_score(y_true_eval, y_pred_eval)\n",
        "\n",
        "print(\"\\n================ RNN TEST METRICS ================\")\n",
        "print(\"MAE :\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAPE:\", mape)\n",
        "print(\"R2  :\", r2)\n",
        "\n",
        "# Save RNN predictions for comparison plots\n",
        "rnn_y_pred_eval = y_pred_eval.copy()\n",
        "rnn_y_true_eval = y_true_eval.copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43dcb302",
      "metadata": {
        "id": "43dcb302"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# REQUIRED: Calculate all 4 metrics\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "rnn_mae  = mae\n",
        "rnn_rmse = rmse\n",
        "rnn_mape = mape\n",
        "rnn_r2   = r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7eb4ceb",
      "metadata": {
        "id": "d7eb4ceb"
      },
      "outputs": [],
      "source": [
        "print(\"\\nRNN Model Performance:\")\n",
        "print(f\"MAE:   {rnn_mae:.4f}\")\n",
        "print(f\"RMSE:  {rnn_rmse:.4f}\")\n",
        "print(f\"MAPE:  {rnn_mape:.4f}%\")\n",
        "print(f\"R² Score: {rnn_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5be5a3",
      "metadata": {
        "id": "8a5be5a3"
      },
      "source": [
        "2.4 Visualize RNN Results\n",
        "TODO: Plot training loss curve\n",
        "TODO: Plot actual vs predicted values\n",
        "TODO: Plot residuals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2.4 Visualize RNN Results\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot training loss curve\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(loss_history, marker=\"o\")\n",
        "plt.title(\"RNN Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot Actual vs Predicted values (first 500 points for clarity)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "n_plot = 500\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(y_true_eval[:n_plot], label=\"Actual\", linewidth=2)\n",
        "plt.plot(y_pred_eval[:n_plot], label=\"Predicted\", alpha=0.8)\n",
        "plt.title(\"Actual vs Predicted Temperature\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Plot Residuals\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "residuals = y_true_eval[:n_plot] - y_pred_eval[:n_plot]\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(residuals, color=\"red\")\n",
        "plt.axhline(0, linestyle=\"--\", color=\"black\")\n",
        "plt.title(\"Residuals (Actual − Predicted)\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z4qli6_l3sNq"
      },
      "id": "z4qli6_l3sNq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411d84b3",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "411d84b3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 3: TRANSFORMER IMPLEMENTATION (5 MARKS)\n",
        "================================================================================\n",
        "\n",
        "REQUIREMENTS:\n",
        "- Build Transformer encoder using Keras/PyTorch layers\n",
        "- MUST add positional encoding to input:\n",
        "  * Custom sinusoidal implementation OR\n",
        "  * Use built-in positional encoding (if framework provides)\n",
        "- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
        "- Use standard training methods\n",
        "- Track initial_loss and final_loss\n",
        "\n",
        "PROHIBITED:\n",
        "- Using pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
        "- Skipping positional encoding entirely\n",
        "\n",
        "GRADING:\n",
        "- Positional encoding added: 1 mark\n",
        "- Transformer architecture properly configured: 2 marks\n",
        "- Training completed with loss tracking: 1 mark\n",
        "- All metrics calculated correctly: 1 mark\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044873c0",
      "metadata": {
        "id": "044873c0"
      },
      "source": [
        "3.1 Positional Encoding Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335f5f6b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "335f5f6b"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(seq_length, d_model):\n",
        "    \"\"\"\n",
        "    Generate sinusoidal positional encodings\n",
        "\n",
        "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
        "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "\n",
        "    Args:\n",
        "        seq_length: length of the sequence\n",
        "        d_model: dimension of the model\n",
        "\n",
        "    Returns:\n",
        "        positional encodings: numpy array of shape (seq_length, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    pe = np.zeros((seq_length, d_model))\n",
        "\n",
        "    positions = np.arange(seq_length).reshape(-1, 1)\n",
        "\n",
        "    div_term = np.exp(\n",
        "        np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model)\n",
        "    )\n",
        "\n",
        "    pe[:, 0::2] = np.sin(positions * div_term)\n",
        "    pe[:, 1::2] = np.cos(positions * div_term)\n",
        "\n",
        "    return pe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83741f21",
      "metadata": {
        "id": "83741f21"
      },
      "source": [
        "3.2 Transformer Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1c4463",
      "metadata": {
        "id": "2f1c4463"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Positional Encoding Module (wraps your 3.1 function for torch use)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = positional_encoding(max_len, d_model)   # uses your function\n",
        "        pe = torch.tensor(pe, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Transformer Encoder Model\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, n_features, d_model, n_heads, n_layers, d_ff, output_size):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.model_type = \"Transformer\"\n",
        "\n",
        "        # Project input → d_model\n",
        "        self.input_projection = nn.Linear(n_features, d_model)\n",
        "\n",
        "        # Positional Encoding (MANDATORY)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # Transformer Encoder stack\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(d_model, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x: (batch, seq_len, n_features)\n",
        "\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f72750",
      "metadata": {
        "id": "06f72750"
      },
      "source": [
        "3.3 Build Your Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db4f26b",
      "metadata": {
        "id": "2db4f26b"
      },
      "source": [
        "TODO: Create Transformer model using PyTorch or Keras\n",
        "Example for PyTorch:\n",
        "transformer_model = TransformerModel(n_features, d_model=64, n_heads=4, n_layers=2, d_ff=256, output_size=prediction_horizon)\n",
        "Example for Keras:\n",
        "transformer_model = build_transformer_model(sequence_length, n_features, d_model=64, n_heads=4, n_layers=2, d_ff=256, output_size=prediction_horizon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3.3 Build Your Transformer Model (PyTorch)\n",
        "# =============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Model hyperparameters\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "n_features = X_train.shape[2]\n",
        "\n",
        "d_model = 64\n",
        "n_heads = 4\n",
        "n_layers = 2      # REQUIRED: stacked encoder layers\n",
        "d_ff = 256\n",
        "\n",
        "output_size = prediction_horizon\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Create Transformer model\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "transformer_model = TransformerModel(\n",
        "    n_features=n_features,\n",
        "    d_model=d_model,\n",
        "    n_heads=n_heads,\n",
        "    n_layers=n_layers,\n",
        "    d_ff=d_ff,\n",
        "    output_size=output_size\n",
        ").to(device)\n",
        "\n",
        "print(transformer_model)"
      ],
      "metadata": {
        "id": "hKQlfLFyfZfN"
      },
      "id": "hKQlfLFyfZfN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5f1bc8c9",
      "metadata": {
        "id": "5f1bc8c9"
      },
      "source": [
        "TODO: Define optimizer and loss\n",
        "For PyTorch: optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001); criterion = nn.MSELoss()\n",
        "For Keras: model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "For PyTorch: define optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Define optimizer and loss (REQUIRED)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    transformer_model.parameters(),\n",
        "    lr=0.001\n",
        ")"
      ],
      "metadata": {
        "id": "c_s2M-KZfiSx"
      },
      "id": "c_s2M-KZfiSx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c00d6c9c",
      "metadata": {
        "id": "c00d6c9c"
      },
      "source": [
        "3.4 Train Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2325454b",
      "metadata": {
        "id": "2325454b"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRANSFORMER MODEL TRAINING\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ed1263",
      "metadata": {
        "id": "26ed1263"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# Track training time\n",
        "transformer_start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "846c1aab",
      "metadata": {
        "id": "846c1aab"
      },
      "source": [
        "TODO: Train your model\n",
        "For Keras: history = transformer_model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "For PyTorch: write training loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Training loop (PyTorch)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "epochs = 20\n",
        "transformer_loss_history = []\n",
        "\n",
        "transformer_model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = transformer_model(xb)\n",
        "\n",
        "        loss = criterion(preds, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    transformer_loss_history.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")"
      ],
      "metadata": {
        "id": "97p8wYOIf_tc"
      },
      "id": "97p8wYOIf_tc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9171c703",
      "metadata": {
        "id": "9171c703"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Training time\n",
        "# ------------------------------------------------------------------\n",
        "transformer_training_time = time.time() - transformer_start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9e89eb",
      "metadata": {
        "id": "3f9e89eb"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Track initial and final loss\n",
        "transformer_initial_loss = transformer_loss_history[0]\n",
        "transformer_final_loss   = transformer_loss_history[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddfc7240",
      "metadata": {
        "id": "ddfc7240"
      },
      "outputs": [],
      "source": [
        "print(f\"Training completed in {transformer_training_time:.2f} seconds\")\n",
        "print(f\"Initial Loss: {transformer_initial_loss:.4f}\")\n",
        "print(f\"Final Loss: {transformer_final_loss:.4f}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3707b1e",
      "metadata": {
        "id": "f3707b1e"
      },
      "source": [
        "3.5 Evaluate Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6e3fa1",
      "metadata": {
        "id": "6a6e3fa1"
      },
      "source": [
        "TODO: Make predictions on test set\n",
        "TODO: Inverse transform if data was normalized\n",
        "TODO: Calculate all 4 required metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3.5 Evaluate Transformer Model\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Make predictions on test set\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "transformer_model.eval()\n",
        "\n",
        "y_preds = []\n",
        "y_trues = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        preds = transformer_model(xb)\n",
        "        y_preds.append(preds.cpu().numpy())\n",
        "        y_trues.append(yb.cpu().numpy())\n",
        "\n",
        "y_pred = np.vstack(y_preds)\n",
        "y_true = np.vstack(y_trues)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# OPTIONAL: Inverse scaling (only if target was normalized)\n",
        "# ------------------------------------------------------------------\n",
        "# Uncomment ONLY if you scaled the TARGET separately\n",
        "# y_pred = target_scaler.inverse_transform(y_pred)\n",
        "# y_true = target_scaler.inverse_transform(y_true)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Multi-step horizon handling\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "if prediction_horizon > 1:\n",
        "    y_pred_eval = y_pred[:, 0]\n",
        "    y_true_eval = y_true[:, 0]\n",
        "else:\n",
        "    y_pred_eval = y_pred.reshape(-1)\n",
        "    y_true_eval = y_true.reshape(-1)\n",
        "\n",
        "\n",
        "transformer_y_pred_eval = y_pred_eval.copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "JlUaM4YdgYWg"
      },
      "id": "JlUaM4YdgYWg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e024b42c",
      "metadata": {
        "id": "e024b42c"
      },
      "outputs": [],
      "source": [
        "transformer_mae  = mean_absolute_error(y_true_eval, y_pred_eval)\n",
        "transformer_rmse = np.sqrt(mean_squared_error(y_true_eval, y_pred_eval))\n",
        "transformer_mape = calculate_mape(y_true_eval, y_pred_eval)\n",
        "transformer_r2   = r2_score(y_true_eval, y_pred_eval)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0126e84",
      "metadata": {
        "id": "c0126e84"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTransformer Model Performance:\")\n",
        "print(f\"MAE:   {transformer_mae:.4f}\")\n",
        "print(f\"RMSE:  {transformer_rmse:.4f}\")\n",
        "print(f\"MAPE:  {transformer_mape:.4f}%\")\n",
        "print(f\"R² Score: {transformer_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d3a7f0",
      "metadata": {
        "id": "c9d3a7f0"
      },
      "source": [
        "3.6 Visualize Transformer Results\n",
        "TODO: Plot training loss curve\n",
        "TODO: Plot actual vs predicted values\n",
        "TODO: Plot attention weights (optional but informative)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Plot Transformer Training Loss Curve\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(transformer_loss_history)\n",
        "plt.title(\"Transformer Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Plot Actual vs Predicted Values\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(y_true_eval, label=\"Actual\")\n",
        "plt.plot(y_pred_eval, label=\"Predicted\")\n",
        "plt.title(\"Transformer: Actual vs Predicted\")\n",
        "plt.xlabel(\"Time Index\")\n",
        "plt.ylabel(\"Target Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Plot Residuals\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "residuals = y_true_eval - y_pred_eval\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(residuals)\n",
        "plt.title(\"Transformer Residuals\")\n",
        "plt.xlabel(\"Time Index\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N3h_9KKhgzfC"
      },
      "id": "N3h_9KKhgzfC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98256c4c",
      "metadata": {
        "id": "98256c4c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 4: MODEL COMPARISON AND VISUALIZATION (Informational)\n",
        "================================================================================\n",
        "\n",
        "Compare both models on:\n",
        "- Performance metrics\n",
        "- Training time\n",
        "- Model complexity\n",
        "- Convergence behavior\n",
        "- Ability to capture long-term dependencies\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e98c47",
      "metadata": {
        "id": "14e98c47"
      },
      "source": [
        "4.1 Metrics Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af1fbf3c",
      "metadata": {
        "id": "af1fbf3c"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total trainable parameters in a PyTorch model\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "rnn_params = count_parameters(rnn_model)\n",
        "transformer_params = count_parameters(transformer_model)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PART 4: MODEL COMPARISON AND VISUALIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': [\n",
        "        'MAE',\n",
        "        'RMSE',\n",
        "        'MAPE (%)',\n",
        "        'R² Score',\n",
        "        'Training Time (s)',\n",
        "        'Parameters'\n",
        "    ],\n",
        "    'RNN (LSTM/GRU)': [\n",
        "        rnn_mae,\n",
        "        rnn_rmse,\n",
        "        rnn_mape,\n",
        "        rnn_r2,\n",
        "        rnn_training_time,\n",
        "        rnn_params\n",
        "    ],\n",
        "    'Transformer': [\n",
        "        transformer_mae,\n",
        "        transformer_rmse,\n",
        "        transformer_mape,\n",
        "        transformer_r2,\n",
        "        transformer_training_time,\n",
        "        transformer_params\n",
        "    ]\n",
        "})\n"
      ],
      "metadata": {
        "id": "fe1D0l6Ag1Yt"
      },
      "id": "fe1D0l6Ag1Yt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a860057",
      "metadata": {
        "id": "2a860057"
      },
      "outputs": [],
      "source": [
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ac551d",
      "metadata": {
        "id": "70ac551d"
      },
      "source": [
        "4.2 Visual Comparison\n",
        "TODO: Create bar plot comparing metrics\n",
        "TODO: Plot predictions comparison (both models vs actual)\n",
        "TODO: Plot training curves comparison"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Metrics and values\n",
        "metrics = ['MAE', 'RMSE', 'MAPE (%)', 'R² Score']\n",
        "rnn_values = [rnn_mae, rnn_rmse, rnn_mape, rnn_r2]\n",
        "transformer_values = [transformer_mae, transformer_rmse, transformer_mape, transformer_r2]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(x - width/2, rnn_values, width, label='RNN (LSTM/GRU)')\n",
        "plt.bar(x + width/2, transformer_values, width, label='Transformer')\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(x, metrics)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LJB5O1L_Y3Md"
      },
      "id": "LJB5O1L_Y3Md",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(y_true_eval[:200], label='Actual', linewidth=2)\n",
        "plt.plot(rnn_y_pred_eval[:200], label='RNN Prediction')\n",
        "plt.plot(transformer_y_pred_eval[:200], label='Transformer Prediction')\n",
        "\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Actual vs Predicted Values')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UDz4YpZSY99A"
      },
      "id": "UDz4YpZSY99A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(rnn_loss_history, label='RNN Training Loss')\n",
        "plt.plot(transformer_loss_history, label='Transformer Training Loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fAbXzL1MZBla"
      },
      "id": "fAbXzL1MZBla",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22896627",
      "metadata": {
        "id": "22896627"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 5: ANALYSIS (2 MARKS)\n",
        "================================================================================\n",
        "\n",
        "REQUIRED:\n",
        "- Write MAXIMUM 200 words (guideline - no marks deduction if exceeded)\n",
        "- Address key topics with depth\n",
        "\n",
        "GRADING (Quality-based):\n",
        "- Covers 5+ key topics with deep understanding: 2 marks\n",
        "- Covers 3-4 key topics with good understanding: 1 mark\n",
        "- Covers <3 key topics or superficial: 0 marks\n",
        "\n",
        "Key Topics:\n",
        "1. Performance comparison with specific metrics\n",
        "2. RNN vs Transformer architecture advantages\n",
        "3. Impact of attention mechanism vs recurrent connections\n",
        "4. Long-term dependency handling comparison\n",
        "5. Computational cost comparison\n",
        "6. Convergence behavior differences\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05be677",
      "metadata": {
        "id": "d05be677"
      },
      "outputs": [],
      "source": [
        "analysis_text = \"\"\"\n",
        "TODO: Write your analysis here (maximum 200 words guideline)\n",
        "\n",
        "Address:\n",
        "1. Which model performed better and by how much?\n",
        "   [Compare specific metrics]\n",
        "\n",
        "2. RNN vs Transformer architecture advantages?\n",
        "   [Discuss sequential processing vs parallel processing]\n",
        "\n",
        "3. Impact of attention mechanism?\n",
        "   [Discuss how attention captures dependencies]\n",
        "\n",
        "4. Long-term dependency handling?\n",
        "   [Compare vanishing gradients vs attention]\n",
        "\n",
        "5. Computational cost comparison?\n",
        "   [Compare training time, parameters]\n",
        "\n",
        "6. Convergence behavior?\n",
        "   [Discuss training stability, loss curves]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05df3e69",
      "metadata": {
        "id": "05df3e69"
      },
      "outputs": [],
      "source": [
        "# REQUIRED: Print analysis with word count\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(analysis_text)\n",
        "print(\"=\"*70)\n",
        "print(f\"Analysis word count: {len(analysis_text.split())} words\")\n",
        "if len(analysis_text.split()) > 200:\n",
        "    print(\"⚠️  Warning: Analysis exceeds 200 words (guideline)\")\n",
        "else:\n",
        "    print(\"✓ Analysis within word count guideline\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f2ce90",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "a4f2ce90"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PART 6: ASSIGNMENT RESULTS SUMMARY (REQUIRED FOR AUTO-GRADING)\n",
        "================================================================================\n",
        "\n",
        "DO NOT MODIFY THE STRUCTURE BELOW\n",
        "This JSON output is used by the auto-grader\n",
        "Ensure all field names are EXACT\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c800bf2e",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "c800bf2e"
      },
      "outputs": [],
      "source": [
        "def get_assignment_results():\n",
        "    \"\"\"\n",
        "    Generate complete assignment results in required format\n",
        "\n",
        "    Returns:\n",
        "        dict: Complete results with all required fields\n",
        "    \"\"\"\n",
        "\n",
        "    framework_used = \"keras\"  # TODO: Change to \"pytorch\" if using PyTorch\n",
        "    rnn_model_type = \"LSTM\"  # TODO: Change to \"GRU\" if using GRU\n",
        "\n",
        "    results = {\n",
        "        # Dataset Information\n",
        "        'dataset_name': dataset_name,\n",
        "        'dataset_source': dataset_source,\n",
        "        'n_samples': n_samples,\n",
        "        'n_features': n_features,\n",
        "        'sequence_length': sequence_length,\n",
        "        'prediction_horizon': prediction_horizon,\n",
        "        'problem_type': problem_type,\n",
        "        'primary_metric': primary_metric,\n",
        "        'metric_justification': metric_justification,\n",
        "        'train_samples': train_samples,\n",
        "        'test_samples': test_samples,\n",
        "        'train_test_ratio': train_test_ratio,\n",
        "\n",
        "        # RNN Model Results\n",
        "        'rnn_model': {\n",
        "            'framework': framework_used,\n",
        "            'model_type': rnn_model_type,\n",
        "            'architecture': {\n",
        "                'n_layers': 0,  # TODO: Number of stacked layers\n",
        "                'hidden_units': 0,  # TODO: Hidden units per layer\n",
        "                'total_parameters': 0  # TODO: Calculate total parameters\n",
        "            },\n",
        "            'training_config': {\n",
        "                'learning_rate': 0.001,  # TODO: Your actual learning rate\n",
        "                'n_epochs': 50,  # TODO: Your actual epochs\n",
        "                'batch_size': 32,  # TODO: Your actual batch size\n",
        "                'optimizer': 'Adam',  # TODO: Your actual optimizer\n",
        "                'loss_function': 'MSE'  # TODO: Your actual loss\n",
        "            },\n",
        "            'initial_loss': rnn_initial_loss,\n",
        "            'final_loss': rnn_final_loss,\n",
        "            'training_time_seconds': rnn_training_time,\n",
        "            'mae': rnn_mae,\n",
        "            'rmse': rnn_rmse,\n",
        "            'mape': rnn_mape,\n",
        "            'r2_score': rnn_r2\n",
        "        },\n",
        "\n",
        "        # Transformer Model Results\n",
        "        'transformer_model': {\n",
        "            'framework': framework_used,\n",
        "            'architecture': {\n",
        "                'n_layers': 0,  # TODO: Number of transformer layers\n",
        "                'n_heads': 0,  # TODO: Number of attention heads\n",
        "                'd_model': 0,  # TODO: Model dimension\n",
        "                'd_ff': 0,  # TODO: Feed-forward dimension\n",
        "                'has_positional_encoding': True,  # MUST be True\n",
        "                'has_attention': True,  # MUST be True\n",
        "                'total_parameters': 0  # TODO: Calculate total parameters\n",
        "            },\n",
        "            'training_config': {\n",
        "                'learning_rate': 0.001,  # TODO: Your actual learning rate\n",
        "                'n_epochs': 50,  # TODO: Your actual epochs\n",
        "                'batch_size': 32,  # TODO: Your actual batch size\n",
        "                'optimizer': 'Adam',  # TODO: Your actual optimizer\n",
        "                'loss_function': 'MSE'  # TODO: Your actual loss\n",
        "            },\n",
        "            'initial_loss': transformer_initial_loss,\n",
        "            'final_loss': transformer_final_loss,\n",
        "            'training_time_seconds': transformer_training_time,\n",
        "            'mae': transformer_mae,\n",
        "            'rmse': transformer_rmse,\n",
        "            'mape': transformer_mape,\n",
        "            'r2_score': transformer_r2\n",
        "        },\n",
        "\n",
        "        # Analysis\n",
        "        'analysis': analysis_text,\n",
        "        'analysis_word_count': len(analysis_text.split()),\n",
        "\n",
        "        # Training Success Indicators\n",
        "        'rnn_loss_decreased': rnn_final_loss < rnn_initial_loss if rnn_initial_loss and rnn_final_loss else False,\n",
        "        'transformer_loss_decreased': transformer_final_loss < transformer_initial_loss if transformer_initial_loss and transformer_final_loss else False,\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694a274d",
      "metadata": {
        "id": "694a274d"
      },
      "outputs": [],
      "source": [
        "# Generate and print results\n",
        "try:\n",
        "    assignment_results = get_assignment_results()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ASSIGNMENT RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(json.dumps(assignment_results, indent=2))\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce921018",
      "metadata": {
        "id": "ce921018"
      },
      "outputs": [],
      "source": [
        "except Exception as e:\n",
        "    print(f\"\\n⚠️  ERROR generating results: {str(e)}\")\n",
        "    print(\"Please ensure all variables are properly defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f84f5c",
      "metadata": {
        "id": "60f84f5c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "FINAL CHECKLIST - VERIFY BEFORE SUBMISSION\n",
        "================================================================================\n",
        "\n",
        "□ Student information filled at the top (BITS ID, Name, Email)\n",
        "□ Filename is <BITS_ID>_rnn_assignment.ipynb\n",
        "□ All cells executed (Kernel → Restart & Run All)\n",
        "□ All outputs visible\n",
        "□ LSTM/GRU implemented with stacked layers\n",
        "□ Positional encoding implemented (sinusoidal)\n",
        "□ Multi-head attention implemented (Q, K, V, scaled dot-product)\n",
        "□ Both models use Keras or PyTorch\n",
        "□ Both models trained with loss tracking (initial_loss and final_loss)\n",
        "□ All 4 metrics calculated for both models (MAE, RMSE, MAPE, R²)\n",
        "□ Temporal train/test split used (NO shuffling)\n",
        "□ Primary metric selected and justified\n",
        "□ Analysis written (quality matters, not just word count)\n",
        "□ Visualizations created\n",
        "□ Assignment results JSON printed at the end\n",
        "□ No execution errors in any cell\n",
        "□ File opens without corruption\n",
        "□ Submit ONLY .ipynb file (NO zip, NO data files, NO images)\n",
        "□ Screenshot of environment with account details included\n",
        "□ Only one submission attempt\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf6d14b",
      "metadata": {
        "id": "bcf6d14b"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "ENVIRONMENT VERIFICATION - SCREENSHOT REQUIRED\n",
        "================================================================================\n",
        "\n",
        "IMPORTANT: Take a screenshot of your environment showing account details\n",
        "\n",
        "For Google Colab:\n",
        "- Click on your profile icon (top right)\n",
        "- Screenshot should show your email/account clearly\n",
        "- Include the entire Colab interface with notebook name visible\n",
        "\n",
        "For BITS Virtual Lab:\n",
        "- Screenshot showing your login credentials/account details\n",
        "- Include the entire interface with your username/session info visible\n",
        "\n",
        "Paste the screenshot below this cell or in a new markdown cell.\n",
        "This helps verify the work was done by you in your environment.\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e01e42",
      "metadata": {
        "id": "16e01e42"
      },
      "outputs": [],
      "source": [
        "# Display system information\n",
        "import platform\n",
        "import sys\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a2fac2",
      "metadata": {
        "id": "c2a2fac2"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ENVIRONMENT INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n⚠️  REQUIRED: Add screenshot of your Google Colab/BITS Virtual Lab\")\n",
        "print(\"showing your account details in the cell below this one.\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}